{"meta":{"title":"Technical blog","subtitle":"IT小白的成长之旅","description":"请乐观，请珍惜","author":"Li Yudong","url":"http://yoursite.com","root":"/"},"pages":[{"title":"archives","date":"2020-08-16T14:58:35.000Z","updated":"2020-08-16T14:58:35.251Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"广告位招租","date":"2020-08-16T15:01:00.000Z","updated":"2020-08-16T15:02:08.971Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-08-16T14:49:27.000Z","updated":"2020-08-16T14:52:47.789Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-08-16T14:45:00.000Z","updated":"2020-08-16T14:48:55.034Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"CSS样式表编程","slug":"CSS样式表编程","date":"2020-09-02T13:58:38.000Z","updated":"2020-09-03T06:31:47.486Z","comments":true,"path":"2020/09/02/CSS样式表编程/","link":"","permalink":"http://yoursite.com/2020/09/02/CSS%E6%A0%B7%E5%BC%8F%E8%A1%A8%E7%BC%96%E7%A8%8B/","excerpt":"","text":"CSS语法两个主要部分：选择器 + 一条或者多条声明；每条声明由一个属性和一个值组成，属性是希望设置的样式属性，每个属性有一个值。CSS声明总是以分号结束，声明总以大括号括起来。 如果要在HTML元素中设置CSS样式，需要在元素中设置id、class选择器。 id选择器可以为标有特定id的HTML元素指定特定的样式，HTML元素以id属性来设置id选择器，CSS中id选择器以#来定义。 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;style&gt;#para1&#123; text-align:center; color:red;&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;p id=&quot;para1&quot;&gt;Hello World!&lt;/p&gt;&lt;p&gt;这个段落不受该样式的影响。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; class选择器用于描述一组元素的样式，区别在于class可以在多个元素中使用， 插入CSS外部样式表：改变一个文件来改变一个站点的外观；每个页面使用 link标签链接到样式表。link标签在文档的头部：而浏览器会从文件mystyle.css中读到样式，并根据它来格式文档。外部样式表可在任何文本编辑器进行编辑，不能包含任何html标签，应该以CSS的扩展名进行保存， 123hr &#123;color:sienna;&#125;p &#123;margin-left:20px;&#125;body &#123;background-image:url(&quot;/images/back40.gif&quot;);&#125; 内部样式表：当单个文档需要特殊格式时，可以使用，可用style标签在文档头部定义内部样式表。 1234567&lt;head&gt;&lt;style&gt;hr &#123;color:sienna;&#125;p &#123;margin-left:20px;&#125;body &#123;background-image:url(&quot;images/back40.gif&quot;);&#125;&lt;/style&gt;&lt;/head&gt; 内联样式： 多重样式：某些属性在不同样式表中被同样的选择器定义，那么属性值将从更具体的样式表中被继承过来。 多重样式优先级：可以在同一个HTML文档内部引用多个外部样式表。优先级如下：内联样式&gt;内部样式&gt;外部样式&gt;浏览器默认样式 CSS背景背景属性用于定义HTML元素的背景： background-color：背景颜色；background-image：背景图像；background-repeat:设置背景不平铺；background-position:背景定位； CSS文本格式格式：颜色color、对齐方式text-align、文本修饰text-decoration、文本大小写转换text-transform、文本缩进text-indent。 字体：两种类型的字体系列名称，通用字体、特定字体。font-family属性设置文本的字体系列， CSS链接四个基本的链接样式实例： a:link - 正常，未访问过的链接 a:visited - 用户已访问过的链接 a:hover - 当用户鼠标放在链接上时 a:active - 链接被点击的那一刻 CSS盒子模型所有HTML元素可以看作盒子，在CSS中，box model这一术语是用来设计和布局时使用的，CSS盒模型本质上是一个盒子，封装周围的HTML元素，包括：边框、边距、填充、实际内容。盒模型允许我们在其他元素和周围元素边框之间的空间放置元素。 不同部分的说明： outline轮廓：绘制于元素周围的一条线，位于边框边缘的外围，起突出元素的作用。 Margin(外边距)** - 清除边框外的区域，外边距是透明的。可以单独改变元素四周边框，也可以一次改变所有属性。 Border(边框)** - 围绕在内边距和内容外的边框，允许一个元素边框的样式和颜色。 Padding(内边距)** - 清除内容周围的区域，内边距是透明的。 Content(内容)** - 盒子的内容，显示文本和图像 CSS分组与嵌套分组选择器：在样式表中有很多具有相同样式的元素，可使用分组选择器，每个选择器用逗号分隔。 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;style&gt;h1,h2,p&#123; color:green;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Hello World!&lt;/h1&gt;&lt;h2&gt;Smaller heading!&lt;/h2&gt;&lt;p&gt;This is a paragraph.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 嵌套选择器：适用于选择器内部的选择器样式， p{ }: 为所有 p 元素指定一个样式。 .marked{ }: 为所有 class=”marked” 的元素指定一个样式。 .marked p{ }: 为所有 class=”marked” 元素内的 p 元素指定一个样式。 p.marked{ }: 为所有 class=”marked” 的 p 元素指定一个样式 1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;style&gt;p&#123; color:blue; text-align:center;&#125;.marked&#123; background-color:red;&#125;.marked p&#123; color:white;&#125;p.marked&#123; text-decoration:underline;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;这个段落是蓝色文本，居中对齐。&lt;/p&gt;&lt;div class=&quot;marked&quot;&gt;&lt;p&gt;这个段落不是蓝色文本。&lt;/p&gt;&lt;/div&gt;&lt;p&gt;所有 class=&quot;marked&quot;元素内的 p 元素指定一个样式，但有不同的文本颜色。&lt;/p&gt; &lt;p class=&quot;marked&quot;&gt;带下划线的 p 段落。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; CSS显示与定位display元素设置一个元素应如何显示，visibility属性指定一个元素可见还是隐藏。 visibility：hidden可以隐藏某个元素，但隐藏的元素仍需占用与未隐藏之前一样的空间，虽被隐藏但仍旧影响布局。display：none可以隐藏某个元素，且隐藏的元素不会占用任何空间。 块元素：h1(标题)、p（段落）、div（文档中的块级元素） 内联元素：span（文档中的内联元素）、a（书签） 可以随时改变元素的种类，从而使页面以不同的方式进行组合。 Position属性：static（默认位置）、relative（相对正常位置的相对位置）、fixed（相对浏览器是固定位置，即使窗口滚动它也不会滚动）、absolute（绝对定位的元素相对于已定位的父元素，如果没有已定位的父元素，则其位置相对于html）、sticky（粘性定位：依赖于用户的滚动，在relative与fixed之间切换）。 元素的定位与文档流无关，所以可以覆盖页面上其他元素，z-index属性指定了一个元素的堆叠顺序，实现重叠。 CSS布局overflow属性用于控制内容溢出元素框时显示的方式，在对应的区间内添加滚动条。 float属性会使元素向左或者向右移动，其周围的元素也会重新排列，往往用于图像或者布局。一个浮动元素会尽量向左或向右移动，直至其外边缘碰到包含框或另一个浮动框的边框。 对齐：1、要水平居中对齐一个元素，可使用margin:auto；并设置到元素的宽度放置它溢出到容器的边缘。","categories":[{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"html","slug":"html前端开发","date":"2020-09-02T03:17:52.000Z","updated":"2020-09-02T13:57:20.272Z","comments":true,"path":"2020/09/02/html前端开发/","link":"","permalink":"http://yoursite.com/2020/09/02/html%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91/","excerpt":"","text":"使用堆的数据结构来对数组进行排序，找出第k大的数据，时间复杂度为O(n) 堆（英语：heap)是计算机科学中一类特殊的数据结构的统称。堆通常是一个可以被看做一棵树的数组对象。堆总是满足下列性质： 堆中某个节点的值总是不大于或不小于其父节点的值； 堆总是一棵完全二叉树。 将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。常见的堆有二叉堆、斐波那契堆等。 堆是非线性数据结构，相当于一维数组，有两个直接后继。 1、基础知识可视化的html页面结构中，只有&lt; body &gt;区域才会在浏览器中显示。 目前在大部分浏览器中，直接输出中文会出现中文乱码的情况，需要在头部将字符声明为UTF-8或GBK。 1&lt;meta charset=&quot;UTF-8&quot;&gt; html元素语法：1、HTML元素以开始标签起始，以结束标签终止；2、元素的内容是开始标签与结束标签之间的内容；3、某些HTML元素具有空内容，空内容在开始标签中进行关闭，以开始标签的结束而结束；4、大多数HTML可拥有属性 嵌套的HTML元素大多数HTML元素可以嵌套，HTML元素可包含其他HTML元素，且HTML文档由相互嵌套的HTML元素组成。 通常不要忘了用结束标签，虽然可以正确显示，但忘记使用结束标签往往会产生不可预料的结果或错误。 HTML标签对大小写不敏感，请一般使用小写标签。 HTML属性元素可设置属性；属性可以在元素中添加附加信息；属性一般描述于开始标签；属性总是以名称、值对的形式出现。比如：name=”value”。 HTML链接由&lt; a &gt;标签定义，链接地址在href属性中指定。 1&lt;a href=&quot;http://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt; HTML格式&lt; hr &gt;标签在HTML页面中创建水平线，用于分隔内容； HTML注释：在开始括号之后紧跟一个叹号，结束括号前不需要 1&lt;!-- 这是一个注释 --&gt; &lt; br &gt;使用br标签：在不产生一个新段落的情况下进行换行； &lt; p &gt;定义一个段落 HTML使用标签&lt; b &gt;与&lt; i &gt;对输出的文本进行格式 1234&lt;b&gt;加粗文本&lt;/b&gt;&lt;br&gt;&lt;br&gt;&lt;i&gt;斜体文本&lt;/i&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;电脑自动输出&lt;/code&gt;&lt;br&gt;&lt;br&gt;这是 &lt;sub&gt; 下标&lt;/sub&gt; 和 &lt;sup&gt; 上标&lt;/sup&gt; HTML链接由&lt; a &gt;标签定义，链接地址在href属性中指定。 target属性：定义被链接的文档在何处显示；id属性：创建一个在HTML文档书签的标记； 1&lt;a href=&quot;http://www.runoob.com&quot;&gt;这是一个链接&lt;/a&gt; HTML头部&lt; head &gt;元素包含了所有的头部标签元素，可以插入脚本、CSS（样式文件）以及各种meta信息 &lt; title &gt; 定义不同文档的标题； &lt; base &gt; 描述了基本的链接地址、链接目标，作为默认链接； &lt; link &gt; 定义了文档与外部资源之间的关系，用于链接到样式表； &lt; style &gt;元素：定义了HTML文档的样式文件引用地址，在该元素中直接添加样式来渲染HTML； &lt; meta &gt;元素：描述了一些基本的元数据，通常用于指定网页的描述，关键词，文件最后的修改时间，作者和其他元数据。 &lt; script &gt; 用于加载脚本文件，如javascript HTML样式 CSS用于渲染HTML元素标签的样式，三种方式添加到HTML中：1、内联样式：在HTML元素中使用style属性；2、内部样式表：在HTML文档头部使用&lt; style &gt; 来包含CSS；3、外部引用：使用外部CSS文件。 最好的方式是外部引用；当特殊的样式需要应用到个别元素时，就可以使用内联样式，在相关标签中使用样式属性。 12&lt;h1 style=&quot;font-family:verdana;&quot;&gt;一个标题&lt;/h1&gt;&lt;p style=&quot;font-family:arial;color:red;font-size:20px;&quot;&gt;一个段落。&lt;/p&gt; 内部样式表：当单个文件需要特别样式时，在&lt; head &gt;部分通过&lt; style &gt;标签定义。 123456&lt;head&gt;&lt;style type=&quot;text/css&quot;&gt;body &#123;background-color:yellow;&#125;p &#123;color:blue;&#125;&lt;/style&gt;&lt;/head&gt; 当样式被应用到很多页面时，外部样式表是理想选择，使用外部样式表，可以通过更改一个文件来修改一整个站点的外观。 123&lt;head&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;mystyle.css&quot;&gt;&lt;/head&gt; HTML图像图像由&lt; img &gt; 标签定义，为空标签，只有属性而无闭合标签，可使用源属性src，其值是图像的URL地址。alt属性用来为图像定义一串预备的可替换文本，在浏览器无法载入图像时，替换文本告诉读者他们失去的信息。 height、width属性用于设置图像的高度与宽度； HTML表格表格由&lt; table &gt;标签来定义，每个表格有若干行（由&lt; tr &gt;标签定义），每行被分割为若干单元格（由&lt; td &gt;标签定义），td指表格数据即数据单元格的内容，border属性来定义表格的边框。 12345678910&lt;table border=&quot;1&quot;&gt; &lt;tr&gt; &lt;td&gt;row 1, cell 1&lt;/td&gt; &lt;td&gt;row 1, cell 2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;row 2, cell 1&lt;/td&gt; &lt;td&gt;row 2, cell 2&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt; HTML列表无序列表用&lt; ul &gt;标签 有序列表始于&lt; ol &gt;标签，每个列表项始于&lt; li &gt;标签，且列表各项会自动使用数字来标记。 自定义列表：项目及其注释的组合；以&lt; dl &gt;标签开始，每个自定义列表项从&lt; dt &gt;开始，每个自定义列表的定义以 &lt; dd &gt;开始。 HTML区块可通过 &lt; div &gt;和&lt; span &gt;将元素组合起来 大多数HTML元素被定义为块级元素、内联元素；块级元素，通常以新行开始和结束，内联元素：显示时不会以新行开始； &lt; div &gt;元素是块级元素，可用于组合其他元素的容器，无特定含义，与CSS一同使用时，可用于对大的内容块设置样式属性；&lt; div &gt;元素的另一个常见作用是文档布局，用table显示表格化数据，用div进行表格定义布局。 &lt; span &gt;元素是内联元素，可用作文本的容器 HTML布局使用div、table元素来创建多列，CSS用于元素定位或为页面创建背景以及色彩丰富的外观。 使用CSS最大的好处是，如果将带啊存放到外部样式表中，那么站点更易于维护，通过编辑单一的文件就可以改变所有页面的布局。 HTML表单用于收集不同类型的用户输入，表单是一个包含表单元素的区域，表单元素允许用户在表单中输入内容，表单标签用&lt; form &gt;来设置。 输入元素input，输入类型由类型属性type定义。text：文本域；password：密码字段；radio：单选按钮；checkbox：复选框；submit：提交按钮 HTML框架、通过使用框架可以在同一个浏览器窗口显示不止一个页面； iframe语法：&lt; iframe src=”URL” &gt;&lt; /iframe &gt; 使用height、width来设置高宽，frameborder属性用于定义iframe表示是否显示边框 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt; &lt;body&gt;&lt;iframe src=&quot;demo_iframe.htm&quot; name=&quot;iframe_a&quot;&gt;&lt;/iframe&gt;&lt;p&gt;&lt;a href=&quot;//www.runoob.com&quot; target=&quot;iframe_a&quot;&gt;RUNOOB.COM&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意：&lt;/b&gt; 因为 a 标签的 target 属性是名为 iframe_a 的 iframe 框架，所以在点击链接时页面会显示在 iframe框架中。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 因为 a 标签的 target 属性是名为 iframe_a 的 iframe 框架，所以在点击链接时页面会显示在 iframe框架中 HTML颜色由16进制符号来定义，RGB三向通道，FF FF FF 141个颜色名称是在HTML和CSS颜色规范定义的，17中表针颜色，124种非标准。 HTML脚本script标签用于定义客户端脚本，比如JavaScript，script元素即可包含脚本语句，也可以通过src属性指向外部脚本文件。JavaScript最常用与图片操作、表单验证以及内容·动态更新。 noscript提供无法使用脚本时的替代内容，可包含普通HTML页面的body元素中能够找到的所有元素。 HTML字符实体其中，某些字符是预留的，比如&gt; 、&lt;；因此需要在源代码中使用字符实体；常用字符实体是不间断空格&amp;nbsp，由于浏览器总会截断HTML页面的空格，只留下一个，因此如果需要在页面中增加空格数量，需要用到&amp;nbsp。 字符实体的名称对大小写敏感。 HTML URLURL是一个网页地址，Web浏览器通过URL从Web服务器请求页面，当点击页面上链接时，对应标签指向万维网上地址。 一个网页地址实例: http://www.runoob.com/html/html-tutorial.html 语法规则: scheme://host.domain:port/path/filename 说明: scheme - 定义因特网服务的类型。最常见的类型是 http host - 定义域主机（http 的默认主机是 www） domain - 定义因特网域名，比如 runoob.com :port - 定义主机上的端口号（http 的默认端口号是 80） path - 定义服务器上的路径（如果省略，则文档必须位于网站的根目录中）。 filename - 定义文档/资源的名称 http：超文本传输协议；https:安全超文本传输协议；ftp：文件传输协议；file：自己计算机上文件。 URL 只能使用 ASCII 字符集. 来通过因特网进行发送。由于 URL 常常会包含 ASCII 集合之外的字符，URL 必须转换为有效的 ASCII 格式。 URL 编码使用 “%” 其后跟随两位的十六进制数来替换非 ASCII 字符。 URL 不能包含空格。URL 编码通常使用 + 来替换空格。 HTML速查列表 2、H5前端开发为了更好地处理今天的互联网应用，HTML5添加了很多新元素及功能，比如: 图形的绘制，多媒体内容，更好的页面结构，更好的形式 处理，和几个api拖放元素，定位，包括网页 应用程序缓存，存储，网络工作者，等。 HTML5 Canvascanvas标签定义图形，只是图形的容器，必须使用脚本来绘制图形。 一个画布在网页中是一个矩形框，通过canvas元素来绘制，指定id属性，height、width属性来定义画布大小，使用style属性来添加边框。 H5内联SVGSVG指可伸缩矢量图形，用于定义用于网络的基于矢量的图形；使用XML格式定义图形，SVG图像在放大或改变尺寸的情况下其图形质量不会有损失，SVG是万维网联盟的标准。 1234567891011&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt; &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; version=&quot;1.1&quot; height=&quot;190&quot;&gt; &lt;polygon points=&quot;100,10 40,180 190,60 10,60 160,180&quot; style=&quot;fill:lime;stroke:purple;stroke-width:5;fill-rule:evenodd;&quot;&gt;&lt;/svg&gt; &lt;/body&gt;&lt;/html&gt; SVG是一种使用XML描述2D图形的语言，而Canvas通过JavaScript绘制2D图形。 SVG基于XML，因此SVG DOM中的每个元素都是可用的，可以为某个元素附加JavaScript事件处理器，在SVG中，每个被绘制的图形均被视为对象，若SVG对象属性发生变化，那么浏览器能自动重现图形； Canvas是逐像素进行渲染，一旦图形被绘制完成，便不会得到浏览器的关注，如果其位置发生变化，则整个场景需要进行重新绘制。 HTML5 MathML标签math，MathML是数学标记语言，基于XML的标准，用于互联网上书写数学符号与公式。 HTML5拖放drag和drop拖放是一种常见的特性，即抓取对象以后拖到另一个位置，在H5中任何元素都可以拖放。 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;style type=&quot;text/css&quot;&gt;#div1 &#123;width:350px;height:70px;padding:10px;border:1px solid #aaaaaa;&#125;&lt;/style&gt;&lt;script&gt;function allowDrop(ev)&#123; ev.preventDefault();&#125; function drag(ev)&#123; ev.dataTransfer.setData(&quot;Text&quot;,ev.target.id);&#125; function drop(ev)&#123; ev.preventDefault(); var data=ev.dataTransfer.getData(&quot;Text&quot;); ev.target.appendChild(document.getElementById(data));&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;拖动 RUNOOB.COM 图片到矩形框中:&lt;/p&gt; &lt;div id=&quot;div1&quot; ondrop=&quot;drop(event)&quot; ondragover=&quot;allowDrop(event)&quot;&gt;&lt;/div&gt;&lt;br&gt;&lt;img id=&quot;drag1&quot; src=&quot;/images/logo.png&quot; draggable=&quot;true&quot; ondragstart=&quot;drag(event)&quot; width=&quot;336&quot; height=&quot;69&quot;&gt; &lt;/body&gt;&lt;/html&gt; 1、设置元素可拖放：把draggable属性设置为true； 2、拖动什么：ondragstart属性调用了一个函数drag(event)，并在函数中用dataTransfer.setData()方法设置被拖数据的数据类型和值； 3、放到何处：ondragover事件规定在何处放置被拖动的数据，默认中无法将数据、元素放置到其他元素中，通过调用ondragover事件的event.preventDefault()方法 4、进行放置：当放置被拖数据时，会发生drop事件，在上面的例子中ondrop属性调用了一个函数，drop(event) 123456function drop(ev)&#123; ev.preventDefault(); var data=ev.dataTransfer.getData(&quot;Text&quot;); ev.target.appendChild(document.getElementById(data));&#125; 调用 preventDefault() 来避免浏览器对数据的默认处理（drop 事件的默认行为是以链接形式打开） 通过 dataTransfer.getData(“Text”) 方法获得被拖的数据。该方法将返回在 setData() 方法中设置为相同类型的任何数据。 被拖数据是被拖元素的 id (“drag1”) 把被拖元素追加到放置元素（目标元素）中 H5地理定位H5视频H5规定了一种通过video元素来包含视频的标准方法， 12345&lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt; &lt;source src=&quot;movie.mp4&quot; type=&quot;video/mp4&quot;&gt; &lt;source src=&quot;movie.ogg&quot; type=&quot;video/ogg&quot;&gt;您的浏览器不支持Video标签。&lt;/video&gt; video与audio元素同样拥有方法、属性、事件，且均可以使用JavaScript进行控制。方法：用于播放、暂停、加载；属性：时长、音量可以被读取或设置；其中DOM事件可以通知用户。下面的例子调用了play()和pause()方法，且使用了paused、width属性。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt; &lt;/head&gt;&lt;body&gt; &lt;div style=&quot;text-align:center&quot;&gt; &lt;button onclick=&quot;playPause()&quot;&gt;播放/暂停&lt;/button&gt; &lt;button onclick=&quot;makeBig()&quot;&gt;放大&lt;/button&gt; &lt;button onclick=&quot;makeSmall()&quot;&gt;缩小&lt;/button&gt; &lt;button onclick=&quot;makeNormal()&quot;&gt;普通&lt;/button&gt; &lt;br&gt; &lt;video id=&quot;video1&quot; width=&quot;420&quot;&gt; &lt;source src=&quot;mov_bbb.mp4&quot; type=&quot;video/mp4&quot;&gt; &lt;source src=&quot;mov_bbb.ogg&quot; type=&quot;video/ogg&quot;&gt; 您的浏览器不支持 HTML5 video 标签。 &lt;/video&gt;&lt;/div&gt; &lt;script&gt; var myVideo=document.getElementById(&quot;video1&quot;); function playPause()&#123; if (myVideo.paused) myVideo.play(); else myVideo.pause(); &#125; function makeBig()&#123; myVideo.width=560; &#125; function makeSmall()&#123; myVideo.width=320; &#125; function makeNormal()&#123; myVideo.width=420; &#125; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; H5新的Input类型color、data(从日期选择器中选择时间)、datetime、datetime-local、email、month、number、range（用于应该包含一定范围内数字值的输入域）、search（定义一个搜索字段）、tel、time、url（自动验证url域的值）、week、 H5新的表单元素datalist：规定输入域的选项列表，该属性规定form、input域应该拥有自动完成功能，当用户在自动完成域中开始输入时，浏览器应该在该域中显示填写的选项，实现一个可输入下拉框的功能。 keygen：提供一种验证用户的可靠方法，规定用于表单的密钥生成器字段；当提交表单时，会生成2个键，一个为私钥、一个为公钥，私钥存储于客户端，公钥被发送至服务器，公钥可用于之后验证用户的客户端证书。 output元素：用于不同类型的输出 12345&lt;form oninput=&quot;x.value=parseInt(a.value)+parseInt(b.value)&quot;&gt;0&lt;input type=&quot;range&quot; id=&quot;a&quot; value=&quot;50&quot;&gt;100 +&lt;input type=&quot;number&quot; id=&quot;b&quot; value=&quot;50&quot;&gt;=&lt;output name=&quot;x&quot; for=&quot;a b&quot;&gt;&lt;/output&gt;&lt;/form&gt; H5表单属性H5的form、input标签添加了几个新属性 新属性： autocomplete novalidate 新属性： autocomplete autofocus form formaction formenctype formmethod formnovalidate formtarget height 与 width list min 与 max multiple pattern (regexp) placeholder required step H5语义元素语义元素：有意义的元素：能够清楚地描述其意义给浏览器和开发者； H5提供了新的语义元素来明确一个Web页面的不同部分。 header（头部区域，定义内容的介绍展示区域）、nav(定义导航链接)、section（定义文档节，页眉、章节）、article（定义独立的内容）、aside（主区域以外的内容，如侧边栏）、figcaption、figure、footer（底部区域） H5的Web存储在本地存储用户的浏览数据，数据以键值对存在，只允许该网页访问使用。 localStorage对象：用于长久保存整个网站的数据，保存数据无过期时间； sessionStorage对象：用于临时保存同一窗口数据，在关闭窗口后会删除数据。 H5 Web SQL数据库引入了一组使用SQL操作客户端数据库的APIs，拥有三个核心方法： 1、openDatabase:使用现有的数据库创建一个数据库对象； 2、transaction：控制一个事务，以及基于这种情况执行提交或回滚； 3、executeSql：执行实际的SQL查询。 H5 Web Workersweb worker是运行在后台的JS，不会影响页面的性能，当HTML页面执行脚本时，页面的状态不可响应直至脚本完成，而Web Worker是运行在后台的JS，独立于其他脚本，不影响页面的性能，可继续做想做的事情。 HTML5 WebSocketWebSocket 是 HTML5 开始提供的一种在单个 TCP 连接上进行全双工通讯的协议。 WebSocket 使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据。在 WebSocket API 中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。 在 WebSocket API 中，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。 现在，很多网站为了实现推送技术，所用的技术都是 Ajax 轮询。轮询是在特定的的时间间隔（如每1秒），由浏览器对服务器发出HTTP请求，然后由服务器返回最新的数据给客户端的浏览器。这种传统的模式带来很明显的缺点，即浏览器需要不断的向服务器发出请求，然而HTTP请求可能包含较长的头部，其中真正有效的数据可能只是很小的一部分，显然这样会浪费很多的带宽等资源。 HTML5 定义的 WebSocket 协议，能更好的节省服务器资源和带宽，并且能够更实时地进行通讯","categories":[{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"自然语言处理","slug":"自然语言处理","date":"2020-08-29T11:02:46.000Z","updated":"2020-08-29T14:17:45.884Z","comments":true,"path":"2020/08/29/自然语言处理/","link":"","permalink":"http://yoursite.com/2020/08/29/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/","excerpt":"","text":"词嵌入one-hot词向量构造起来很容易，但并不是一个含选择，因为其并不能准确地表达不同词之间的相似度，word2vec工具提出解决了上述问题，将每个词表示成一个定长的向量，并使这些向量能较好地表达不同词之间的相似和类比关系，包括跳字模型和连续词袋模型。 跳字模型跳字模型假设：基于某个词来生成它在文本序列周围的词。 在该模型中，每个词被分为2个d维向量，用来计算条件概率，假设该词在词典索引为i，当它为中心词时向量表示为Vi，而它为背景词时向量表示为Ui。 设中心词Wc在词典中索引为c，背景词Wo在词典中索引为o，故给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到： 跳字模型的参数是每个词所对应的中心词向量和背景词向量，训练中一般使用最大似然函数来学习模型参数 连续词袋模型与跳字模型不同的是，连续词袋模型假设：基于某中心词在文本序列前后的背景词来生成该中心词。因为连续词袋模型的背景词有很多个，因此将这些取平均，然后使用和跳字模型一样的方法来计算条件概率。 同样，连续词袋模型的最大似然估计等价于最小化损失函数。 近似训练跳字模型的核心在于，使用softmax运算得到给定中间词Wc生成背景词Wo的条件概率，，该条件概率对应的对数损失； 由于softmax运算考虑背景词可能是词典中任一词，以上损失包含了词典大小数目的项的累加。因此每次梯度计算可能开销过大，有下面两种方法来进行近似训练。 负采样：修改了原来的目标函数，使用相互独立事件来构造损失函数，其训练中每一步梯度计算开销与采样的噪声词的个数线性相关。 层序softmax：使用了二叉树这一个数据结构，树的每个叶结点代表词典中的每个词，并根据根节点到叶节点的路径来构造损失函数，每一步的梯度计算开销与词典大小的对数相关。 word2vec的实现预处理数据集PTB是常用的语料库 1、建立词语索引：将词映射到整数索引 2、二次采样：：文本中一般会出现一些高频词，而在背景窗口中，与高频词一起出现会更有益。故训练词嵌入模型时可以对词进行二次采样，即每个索引词都有一定概率被丢弃。 3、提取中心词与背景词：我们将与中心词距离不超过背景窗口大小的词作为背景词，定义函数提取出所有中心词和它们的背景词。它每次在整数1与max_window_size之间随机均匀采样一个整数作为背景窗口大小。 负采样读取数据集使用随机小批量来读取数据集，小批量读取函数batchify，其输入data是一个长度为批量大小的列表，其中每个元素分别包含中心词center、背景词context、噪声词negativ，其返回的小批量数据符合我们需要的格式。 跳字模型嵌入层：获取词嵌入的层称为嵌入层，在Gluon中可以通过创建nn.Embedding实例得到。其权重为一个矩阵，行数为词典大小、列数为每个词向量的维度。嵌入层输入为词的索引，返回为权重矩阵的第i行作为它的词向量。 小批量乘法：batch_dot对两个小批量中的矩阵一一做乘法。 跳字模型前向计算：输入包含中心词索引center以及连结的背景词与噪声词索引contexts_and_negatives。 定义损失函数：使用Gluon的二元交叉熵函数 子词嵌入fastText英语单词通常由其内部结构和形成方式，而在word2vec中，我们并没有直接利用构词学中信息，而在fastText中，每个中心词被表示为子词的集合，利用 全局向量的词嵌入GloVe文本分类情感分析：使用循环神经网络文本分类是自然语言处理的一个常见任务，将一段不定长的文本序列变换成文本的类别。 子问题：使用文本情感分析来分析文本作者的情绪，即情感分析。 文本情感分析：使用卷积神经网络textCNN其实，我们也可以将文本看作一维图像，从而可以使用一维卷积神经网络来捕捉临近词之间的关联， 编码器-解码器Seq2seq前面都是表征并变换了不定长的输入序列，但在自然语言处理的很多应用中，输入、输出都可以是不定长序列，此时可用编码、解码器或Seq2seq模型。两个模型的本质都用到了两个循环神经网络，分别为编码器、解码器。+","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"计算机视觉","slug":"计算机视觉","date":"2020-08-28T10:44:52.000Z","updated":"2020-08-29T11:01:56.399Z","comments":true,"path":"2020/08/28/计算机视觉/","link":"","permalink":"http://yoursite.com/2020/08/28/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/","excerpt":"","text":"在卷积神经网络中介绍了计算机视觉领域常用的深度学习模型，并实践了简单的图像分类。先描述目标检测的流程与方法，再使用全卷积网络对图像做语义分割，之后用样式迁移技术生成图像。 图像增广1、扩大样本数据集；2、随机改变训练样本可降低模型对某些属性的依赖。两者均为了提高模型的泛化性。 常用方法：翻转和裁剪、变化颜色、叠加多个图像增广方法； 为了在预测时获得准确结果，图像增广通常仅用于训练集，而不在预测时使用含随机操作的图像增广。 Gluon数据集提供的transform模块中，transform_first函数将图像增广应用在每个训练样本（图像和标签）的第一个元素，即图像之上。 用增广后图像训练模型：1、定义try_all_gpus函数，获取所有能用的GPU； 2、定义辅助函数_get_batch将小批量数据样本batch划分，并复制到ctx变量所指定的各个显存上； 3、定义evaluate_accuracy函数来评价模型的分类准确性，该函数通过辅助函数_get_batch使用ctx变量所包含的所有GPU来评价模型； 4、定义train函数使用多GPU训练并评价模型； 5、最终定义train_with_data_aug函数，来使用图像增广来训练模型；该函数获取所有GPU，并将Adam算法作为训练优化算法，之后将图像增广应用于训练数据集上，最后调用刚定义的train函数训练并评价模型。 12345678def train_with_data_aug(train_augs, test_augs, lr = 0.001): batch_size, ctx, net = 256, try_all_gpus(), d2l.resnet18(10) net.initialize(ctx=ctx, init=init.Xavier()) trainer = gluon.Trainer(net.collect_params(), &#x27;adam&#x27;, &#123;&#x27;learning_rate&#x27;: lr&#125;) loss = gloss.SoftmaxCrossEntropyLoss() train_iter = load_cifar10(True, train_augs, batch_size) test_iter = load_cifar10(False, test_augs, batch_size) train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs=10) 图像微调由于收集数据所需要的成本较高，应用迁移学习：将从源数据集学到的知识迁移到目标数据集上。例：可从图形数据集训练的模型中抽取较通用的模型特征，边缘、纹理、形状、物体组成识别等等。 微调时常用的一种迁移学习技术，当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力，步骤包括： 1、在源数据集上预训练一个神经网络模型，即源模型； 2、创建一个新的神经网络模型，即目标模型，其复制了源模型上除了输出层以外的所有模型设计与参数。假设该模型参数包含了源数据集上学习到的知识，且同样适用于目标数据集。 3、为目标模型添加一个输出大小为目标数据集类别个数的输出层，并初始化该层的模型参数。 4、在目标数据集上训练目标模型，将从头训练输出层，而其余层的参数将会基于源模型的参数微调得到。 热狗识别基于一个小数据集对在ImageNet数据集上训练好的ResNet模型进行微调。 1、获取数据集； 2、定义和初始化模型：使用在ImageNet数据集上预训练的ResNet-18作为源模型，该源模型实例含有两个成员变量，即features和output。前者包括模型输出层以外的所有层，后者为模型的输出层，这样的划分方便微调除输出层以外所有层的模型参数。 3、微调模型：先定义一个使用微调的训练函数train_fine_tuning，以便多次调用。 123456789def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5): train_iter = gdata.DataLoader(train_imgs.transform_first(train_augs), batch_size, shuffle=True) test_iter = gdata.DataLoader(test_imgs.transform_first(test_augs), batch_size) ctx = d2l.try_all_gpus() net.collect_params().reset_ctx(ctx) net.hybridize() loss = gloss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), &#x27;sgd&#x27;, &#123;&#x27;learning_rate&#x27;: learning_rate, &#x27;wd&#x27;: 0.001&#125;) d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs) 一般来说，微调参数会使用较小的学习率；而从头训练输出层可以使用较大学习率。 目标检测与边界框在图像分类任务里，假设只有一个主体目标；而目标检测往往是图像中有多个感兴趣的目标。 目标检测算法通常会在输入图像中采样大量的区域，，然后判断是否包含感兴趣的目标，并调整区域边缘从而更准确地预测目标的真是边界框。 锚框：以每个像素为中心生成多个大小和宽高比不同的边界框。 交并比：（若某个锚框较好地覆盖了图像的狗，那么较好该如何量化）直观的方法是，衡量锚框与真实边界框间的相似度Jaccard系数可以衡量两个集合的相似度，Jaccard系数等于二者交集大小除以二者并集大小。 在训练集中，将每一个锚框视为一个训练样本，为了训练目标检测模型，需为每个锚框标注两个标签：1、锚框所含目标的类别；2、真实边界框相对锚框的便宜量offset。 在目标检测的训练集中，每个图像已经标注了真实边界框的位置及所含目标的类别，那么生成锚框后，如何为锚框分配与其相似的真实边界框呢？ 分配真实边界框1、锚框有Na个，真实边界框有Nb个，定义矩阵为Na X Nb，其第i列第j行的元素为锚框Ai与真实边界框Bj的交并比。则通过不停找出矩阵最大元素，且每找出一个元素则丢弃该行列的元素，直至矩阵丢弃完，只剩Na - Nb个锚框。 2、遍历剩下的锚框，只有该交并比大于预先设定的阈值时，才为锚框分配真实边界框Bj。 3、如果一个锚框A被分配了真实边界框B，将A的类别设为B的类别，并根据B和A的中心坐标的相对位置以及两个框的相对大小为锚框A标注偏移量。如果一个锚框没有被分配真实边界框，需将该锚框的类别设为背景，称为负类锚框。 4、通过contrib.nd模块中的MultiBoxTarget函数来为锚框标注偏移量和类别。该函数将背景设定为0，并从令0开始的目标类别的整数索引自加1，并通过expand_dims函数为锚框和真实边界添加样本维，并构造形状为（批量大小，包括背景的类别个数，锚框数）的任意预测结果。 非极大值抑制当锚框数量较多时，同一目标可能输出较多相似的。用非极大值抑制来移除：对一个预测边界框B，模型会计算其各个类别的预测概率，其中最大概率对应的类别即B的预测类别，且在同一图像上将预测类别置信度从高到低排列，得到列表L。从L中选取置信度最高的预测边界框B1为基准，将与B1交并比大于某阈值的从L中移除，阈值为预定的超参数，此时L保留了置信度最高的边界框并移除了与之相似的其他预测边界框。 多尺度目标检测 如果以图像每个像素中心都生成锚框，很容易生成过多锚框而造成计算量过大，方法一：在输入图像中均匀采样一小部分像素，并以采样的像素为中心生成锚框。之后既然我们已经在不同尺度下生成了不同大小的锚框，相应的需要在不同尺度下检测不同大小的目标，基于卷积神经网络有如下的方法： 在某尺度下，假设我们根据Ci张形状为h X w的特征图生成h X w组不同中心的锚框，且每组锚框的个数为a。 假设这里的Ci张特征图为卷积神经网络根据输入图像做前向运算所得的中间输出，根据感受野的定义，特征图在相同位置的Ci个单元在输入图像的感受野相同且表征了同一感受野内的输入图像信息。因此我们将这Ci个单元变换为该位置为中心生成的a个锚框的类别和偏移量，故本质上使用感受野内的信息来预测锚框。 因此不同大小的感受野用于检测不同大小的目标，可通过设计网络来控制输出层感受野大小，从而分别用来检测不同大小的目标。 单发多框检测由一个基础网络块和若干多尺度特征块串联而成。其中网络块用于从原始图像中抽取特征，因此一般会选择常用的深度卷积神经网络，例如：在分类层之前截断的VGG、或者用ResNet替代。 设计基础网络，使其输出的高宽较大，这样一来基于该特征图生成的锚框数量较多，用于检测较小目标；接下来每个多尺度特征块将上一层提供的特征图的高、宽减小，使感受野变广阔，这样越靠顶部其特征图越小，生成锚框越少，适合检测尺寸大的目标。借此，单发多框检测是一个多尺度的目标检测。 类别预测层如果用全连接层作为输出，容易导致模型参数过多，故像NIN一样使用卷积层的通道进行输出类别的预测，来降低模型复杂度。即使用一个保持输入高、宽的卷积层，使输入、输出的空间坐标一一对应 边界预测层设计与类预测层类似，需要为每个锚框预测4个偏移量。 连接多尺度的预测由于每个尺度的特征图形状与锚框个数都可能不同，因此不同尺度预测输出形状可能不同。需要将他们变形成统一的格式并将多尺度的预测连结，从而让后续的计算更简单。 高、宽减半块为了能多尺度地检测目标，需要定义高宽减半块，其串联了两个填充为1的3X3卷积层和步幅为2的2X2最大池化层，卷积层不改变特征图形状，而后面池化层将特征图的高、宽减半。 基础网络块用于在原始图像中抽取特征，此处串联3个高、宽减半块，并将通道数翻倍，则当输入图像形状为256X256时，基础网络块的输出特征图的形状为32X32。 完整的模型单发多框检测一共包括5个模块，每个模块即生成锚框，又来预测锚框的类别与偏移量。第一模块为基础网络块，二至四模块为高宽减半块，第五模块使用全局最大池化层将高和宽降到1。 单发多框检测训练模型1、读取数据集并初始化；2、定义损失函数与评价函数：一、有关锚框类别的损失，图像分类问题一般使用的：交叉熵函数 二、有关正类锚框偏移量的损失：预测偏移量是一个回归问题，因此不用平方损失，而用L1范数损失，即预测值与真实值之间差的绝对值。 3、训练模型在模型的前向计算过程中生成多尺度的锚框anchors，并为每个锚框预测类别cls_preds和偏移量bbox_preds，之后根据标签信息Y为生成的每个锚框标注类别和偏移量。最后，根据这两者值来计算损失函数。 4、预测目标在预测阶段，我们读取图像并变换尺寸，转换为卷积层所需的四维格式，通过MultiBoxDetection函数根据锚框及其预测偏移量得到预测边界框，并通过非极大值抑制移除相似的预测边界框；最后，将置信度不低于0.3的边界框筛选为最终输出。 区域卷积神经网络R-CNNR-CNN首先对图像选取若干提议区域，并标注它们的类别和边界框，之后用卷积神经网络对每个提议区域做前向运算来抽取特征。 1、对输入图像进行选择性搜索，来选取多个高质量的提议区域，通常在多个尺度下选取，并标注类别与真实边界框； 2、选取一个预训练的卷积神经网络，并将其在输出层之前截断，并将每个提议区域变形为网络所需要的尺寸，并通过前向计算输出抽取的提议区域特征； 3、将每个提议区域的特征连同其标注的类别作为一个文本，训练多个支持向量机对目标进行分类，其中每个支持向量机用来判断样本是否属于一个实例； 4、将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。 FAST R-CNNR-CNN抽取的独立特征常有大量重复计算，利用FAST R-CNN进行简化， FASTER R-CNN将选择性搜索替换成区域提议网络，从而减少提议区域的生成数量，以达到较精确的目标检测结果。 Mask R-CNN当训练数据还标注了每个目标在图像上的像素级位置，那么Mask R CNN模型能有效利用这些详尽的标注信息 语义分割和数据集语义分割问题：关注如何将图像分割成属于不同语义类别的区域，且均为像素级，相比于锚框更加精确。 图像分割问题：利用像素间相关性将图像分割成若干区域，且训练时并不需要像素有关的标签信息，预测时也无法保证希望得到的语义。 实例分割问题：研究如何识别图像中各个目标实例的像素级区域，不仅要区分语义，还要区分不同目标实例，比如：区分两条同样语义的狗。 Pascal VOC2012数据集 由于语义分割的输出图像和标签在像素上一一对应，所以将图像随机裁剪成固定尺寸而不是缩放。 全卷积网络FCNFCN实现了从图像像素到像素类别的变换；FCN通过转置卷积层，将中间层特征图的高、宽变换回输入图像的尺寸，从而令预测结果与输入图像在空间维上一一对应。 转置卷积层构造模型1、先使用卷积神经网络来抽取图像特征； 2、通过1X1卷积层将通道数变换成类别个数； 3、通过转置卷积层，将特征图的高、宽变换为输入图像的尺寸，使模型输出与输入图像的高、宽相同，并在空间位置一一对应，最终输出的通道中包含了该空间位置像素级别的类别预测； 样式迁移使用卷积神经网络自动将某图像中的样式应用在另一图像上，两张输入图像：内容图像、样式图像。 具体实施 1、初始化合成图像，一般初始化成内容图像，该图像便是样式迁移过程中需要迭代的模型参数。 2、选择一个预训练的卷积网络来抽取图像的特征，其中模型参数在训练时无需更新，深度神经网络凭借多个层级逐级抽取图像的特征，可以选择其中某些层的输出作为内容特征； 3、正向传播计算样式迁移的损失函数，通过反向传播迭代模型参数，即不断更新合成图像。 预处理和后处理图像预处理：在RGB三个通道分别做标准化，将结果变换成输入形式； 后处理：将输出图像中的像素值还原回标准化之前值； 抽取特征使用基于ImageNet数据集训练的VGG-19模型来抽取图像特征； 定义损失函数内容损失：利用平方误差函数衡量合成图像与样式图像在内容上差异； 样式损失：利用平方误差函数衡量合成图像与样式图像在样式上差异； 总变差损失：用于降噪，使合成图像中噪点（特别亮或特别暗的颗粒像素）减少。 损失函数为以上三者的加权和。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"优化算法与计算性能","slug":"机器学习优化算法","date":"2020-08-28T04:18:39.000Z","updated":"2020-08-28T10:43:40.737Z","comments":true,"path":"2020/08/28/机器学习优化算法/","link":"","permalink":"http://yoursite.com/2020/08/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/","excerpt":"","text":"优化与深度学习一般会预定义一个损失函数，再使用优化算法试图将其最小化，这样的损失函数通常被称为优化问题的目标函数，通常只考虑最小化目标函数。由于优化算法的目标函数通常是一个基于训练集的损失函数，故优化目的在于降低训练误差，而深度学习目的在于降低泛化误差，因此需要注意过拟合问题。 很多优化问题并不存在解析解，因此需要通过优化算法有限次迭代模型参数来尽可能降低损失函数值。 局部最小值当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成0，因此最终迭代可能只令目标函数局部最小化而非全局最小化。 鞍点在二维空间函数中，f(x,y) = x^2 - y^2，鞍点位置是x = 0处。且在图的鞍点位置，目标函数在x轴方向上是局部最小值，但在y轴方向上是局部最大值。 假设函数的输入为k维向量，输出为标量，则其海森矩阵有k个特征值；可通过该函数在对应点，其海森矩阵的特征值的正负，来判断该点为：（特征值全为负）局部最大值、（特征值全为正）局部最小值，还是（特征值有正有负）鞍点。 而通过随机矩阵理论可知：对一个大的高斯随机矩阵来说，任一特征值为正或负的概率均为0.5，故局部最小、最大值的可能性均为(0.5)^k，目标函数的鞍点比局部最值更常见。 梯度下降与随机梯度下降一维梯度下降：通过用X - nf*(x)来代替x的方法，利用该式子不断迭代x，直到达到停止条件（一般为f’(x) ^2已经足够小，或者迭代次数已到达某个值）。其中正数n通常叫做学习率，为超参数，需人工设定。学习率过小：x更新缓慢，需要更多次迭代；学习率过大：可能会导致taylor展开的不等式不一定成立，迭代x不一定减小f(x)的值。 多维梯度下降：方向导数给出了x沿所有可能方向的变化率，为了最小化f，希望能找到f能被下降最快的方向，故利用梯度下降算法不断降低f的值。 随机梯度下降：n为训练数据样本数，x为模型的参数向量。则如果使用梯度下降时，会使用各个样本的平均作为，每次自变量迭代的计算开销为O(n)，随n线性增长，因此若样本数大时，每次迭代的计算开销高。而随机梯度下降减少了计算开销，在每次迭代中随机均匀采样样本索引来计算梯度，从而减少每次迭代的开销。 小批量随机梯度下降在每次迭代，梯度下降用整个训练集来计算梯度，而小批量梯度随机下降，利用随机均匀采样一个由样本索引组成的小批量B。 且由于随机采样得到梯度的方差在迭代过程中无法减小，因此实际中，小批量随机梯度下降的学习率需要在迭代过程中自我衰减。 在Gluon中可用创建Trainer实例来调用调优算法。 动量法梯度下降又称最陡下降：自变量在当前位置下降最快的方向，在每次迭代中梯度下降根据自变量当前位置沿着梯度来更新自变量，然而，若自变量的迭代方向仅仅取决于自变量当前位置，可能会带来问题。 在二维或者多维的变量中，梯度下降往往难以同时兼顾学习率与确保f(x)下降；需要确保学习率较小，从而避免自变量在竖直方向越过函数最优解，但会因此导致向最优解移动缓慢。 动量法：设时间步t的自变量为Xt，学习率为Nt，动量法对每次迭代的步骤做出以下修改：Vt &lt;- yV(t-1)；Xt &lt;- X(t-1) - Vt。y为动量超参数，范围在[0，1） 指数加权移动平均： 由指数加权平均理解动量法： 相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且在超参数中多了动量超参数。 在Gluon中，需要在Trainer实例中通过momentum来指定动量超参数，即可使用动量法。 AdaGrad算法动量法依赖指数加权移动平均，使得自变量的更新方向更加一致，从而降低自变量在梯度较大的维度发散的可能。 而AdaGrad算法根据自变量在每个维度的梯度大小，来调整各个维度的学习率，从而避免统一的学习率难以适应所有维度的问题。 AdaGrad算法会使用一个小批量随机梯度Gt按元素平方的累加变量St： Gluon中使用名称为”adagrad”的Trainer实例来调用该算法训练模型。 RMSProp算法AdaGrad算法在迭代后期由于学习率过小，可能比较难找一个有用的解：因此用RMSProp算法改良后。 不同于AdaGrad算法里状态变量St是截至时间步t所有小批量随机梯度Gt按元素平方和。RMSProp算法将这些梯度按元素平方做指数加权移动平均，即 Gluon中使用名称为”rmsprop”的Trainer实例来调用该算法训练模型，且超参数由gammal指定。 还有AdaDelta算法、Adam算法 深度学习计算性能命令式和符号式混合编程之前一般使用Sequential类来串联多个层，先为了使用混合式编程，使用HybridSequential类来替换Sequential类。 异步运算MXNet使用异步运算来提升性能，通过前端线程与后端线程的交互进行异步运算：前端线程无需等待当前指令从后端线程返回结果就继续执行后面的指令。 但同样异步运算会占据额外的内存：由于深度学习模型往往比较大，且内存资源通常有限，因此在训练模型时通常使用同步函数，而不用异步运算。 自动并行计算MXNet后端会自动构建计算图，依据该图，系统会自动知道所有计算的依赖关系。 包括CPU与GPU的并行计算、多GPU计算、数据并行。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"C语言程序设计","slug":"C语言程序设计","date":"2020-08-25T12:54:02.000Z","updated":"2020-09-02T04:49:17.469Z","comments":true,"path":"2020/08/25/C语言程序设计/","link":"","permalink":"http://yoursite.com/2020/08/25/C%E8%AF%AD%E8%A8%80%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"一、基础知识在命令行中进行编译运行12345678~/project ls -l #执行命令ls，用于列出当前所在计算机存储位置中的文件和目录，并为其配置参数-l-rw-r--r-- 1 user user 112 Aug 25 20:48 main.c ~/project gcc -o program main.c #gcc为编译器名称，用该命令告诉gcc，将main.c的代码文件编译成名为program的可执行文件~/project ls -l-rw-r--r-- 1 user user 112 Aug 25 20:48 main.c -rwxrwxr-x 1 user user 8512 Aug 25 21:35 program #目录中新增了program的可执行文件./program #执行程序 变量相关变量名：由大、小写字母，下划线，数字组成；数字不能开头；不能是有特定含义的保留字。 作用域：变量声明语句之后，包裹了它声明语句的最内一层{}中，且一个变量在其作用域内仅能声明一次，但能够赋值多次，只要是在其作用域内的赋值，均起作用。 这些在结构化语句的内部的变量的作用域为结构化语句内部。注意，对于switch中在case内部定义的变量的作用域就是在当前case。我们可以简单理解为在大括号内部定义的变量，其作用域就是在当前大括号中，在当前大括号外部无效。对于循环嵌套和分支嵌套程序来说都是一样的。关于这一点不再赘述。 注意，当一个嵌套结构中出现两个不同作用域的变量时，变量的名称可以相同，在使用时以其小作用域为准。 与局部变量相对的就是全局变量，我们把定义在函数外部的变量称为全局变量，这些变量的作用域为整个程序，也就是所有的函数和结构化语句都能使用它们。 甚至多个源文件一起编译时，全局变量在其他文件中也能够生效需要用extern关键字在函数外部声明一个文件外部变量。 递归问题在头递归的实现中，我们在进行下一层的调用前，没有进行计算，只有在下一层的返回之后，我们才完成了这一层的运算。 在尾递归的实现中，我们在进行下一层的调用前，会先进行计算，而在最终一般条件满足时，会将计算的结果逐层直接返回。 声明与实现分离如果采用直接定义的方式来创造函数，则需要时刻关注它们之间依赖关系，并正确排序，不现实。需要进行声明与实现的分离，从而简化这一过程。 只需要在需要用到该函数前进行声明即可，定义可放在执行程序的后面位置，函数声明时可以不写函数参数名，只写参数类型。 变量地址做函数参数swap函数中，由于该函数定义的形式参数a,b作用域有限，因此swap函数内的交换并不会影响main函数中x，y大的值。此时，需修改成传入参数x、y的地址，直接对其地址进行操作。 123456789101112void swap(int *a, int *b);int main()&#123; int x,y; swap(&amp;x, &amp;y);&#125;void swap(int *a, int *b)&#123; int temp; temp = *a; *a = *b; *b = temp;&#125; 函数地址做函数参数C语言中函数与变量类似，也有其自己的内存地址，但函数不能像变量一样可以进行值传递，在想要将函数作为另一个函数的参数进行传递时，需要传递它的地址。 123int g(float (*f)(int), int a)&#123; return f(a);&#125; 上面这种情况，函数g需要有一个形式参数来接收函数地址，其第一个参数需要一个返回值类型为float且有一个int类型参数的函数；第二个参数就是普通的int类型值 其实直接写f(a)和float (*f)(int)来调用其地址本质上是一样的。因此声明时需要如上所示去取函数的地址，而调用时，直接函数名与变量一起传入也是可以的。g(f(x), a)。 有关代码风格的空格哪些使用空格的地方： 1、+、-、&gt;、==、|、&amp;&amp;等双目运算符前后； 2、if、switch、for、while等关键字，函数名和之后的左小括号之间； 3、不在行尾的逗号、分号之后，例如for循环中的分号之后； 4、必须加空格的情况：如return后面不加空格就会报语法错误的情况。 数组相当于定义了一系列地址相邻的元素，与取变量地址的方式一致，可以通过&amp;radius[1]的方式取得数组radius在索引位置1元素的地址。在C语言中，对一个元素的地址加上位移值n得到的就是这个元素往后数n后所在元素的地址。 且一般来说，在地址上进行运算的方式访问数组的效率比利用索引更快，例如：你只希望访问数组中每一个元素一次时，可用while循环内使用地址上运算的方式，使用数组中每一个元素的值，而无需关心数组的索引是谁。 123int *p_radius;p_radius = &amp;radius[0];//这里*(p_radius + 1)或者（&amp;radius[0] + 1）都会得到radius[1]元素的地址。 字符串的本质是数组字符串实际上是一个元素为字符的数组，例如“Hello”由五个字母字符与一个空字符\\0组成；任何字符串的内部表示都会以空字符‘\\0’作为结尾，故可以以此方式找到字符串结尾。 同样，在C中提供字符数组初始化的简化方式： 1char string[] = &quot;Hello&quot;; 字符串更严谨应该被称为 字符串字面量，其表现为一对双引号包裹的0个或者多个字符；字面量并非仅包含字符串常量 1234int a;a = 1234;//语句中的1234其实就是一个整数型字面量，其实是将一个整数型字面量的值放入了变量中作为值。在字面量后往往需要增加一个后缀标记类型。L：长类型；U：无符号类型；F：浮点类型。//除了十进制，也可用其他进制表示字面量。 除了用字符数组存储字符串，也可声明一个用于存储字符地址的变量操作字符串 123456789101112#include &lt;stdio.h&gt;//string的地址是内存栈区的地址；string2则是直接关联到&quot;Hello&quot;字符串字面量在内存中字面量池的地址。int main() &#123; char string[] = &quot;Hello&quot;; printf(&quot;%s\\n&quot;, string); char *string2 = &quot;Hello&quot;; printf(&quot;%s\\n&quot;, string2); printf(&quot;%p\\n&quot;, &amp;string); //0x7fff09dd0480 printf(&quot;%p\\n&quot;, string2);//0x4allc4 printf(&quot;%p\\n&quot;, &amp;&quot;Hello&quot;);//0x4allc4 return 0;&#125; 二、简单算法牛顿迭代法多数方程不存在求根方式，因此用牛顿法寻找方程的近似跟，时间复杂度为log n。 步骤：1、确定迭代变量；2、建立迭代关系式；3、对迭代过程进行控制。 123456789101112131415161718192021222324252627282930313233#include &lt;stdio.h&gt;#include &lt;math.h&gt;#define EPSILON 1e-6double f(double x) &#123; return 2 * pow(x, 3) - 4 * pow(x, 2) + 3 * x - 6;&#125;double f_prime(double x) &#123; return 6 * pow(x, 2) - 8 * x + 3;&#125;double h(double x)&#123; return pow(x,3) - 4 * pow(x,2) + 3 * x - 6;&#125;double h_prime(double x)&#123; return 3 * pow(x,2) - 8 * x + 3;&#125;double newton(double (*fp)(double), double(*fp_prime)(double)) &#123; double x = 1.5; while (fabs(fp(x)) &gt; EPSILON)&#123; x = x - fp(x) / fp_prime(x); &#125; return x;&#125;int main() &#123; printf(&quot;%g\\n&quot;, newton(f, f_prime)); printf(&quot;%g\\n&quot;, newton(h, h_prime)); return 0;&#125; 二分法二分法同样是一个求方程近似跟的方法，在使用二分法近似求解时，先设定一个迭代区间，且区间两边自变量x对应的F(X)是异号的，之后计算两端中点位置x对应的f(x)，再更新迭代区间，并确保迭代区间两端x对应的函数值还是异号，重复过程直至中点x对应的f(x)小于某个值。 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;math.h&gt;#define EPSILON 1e-7double bisection(int p, int q, double (*func)(int, int, double));double f(int p, int q, double x);int main() &#123; int p; int q; scanf(&quot;%d%d&quot;, &amp;p, &amp;q); printf(&quot;%.4f\\n&quot;, bisection(p, q, f)); return 0;&#125;double bisection(int p, int q, double (*func)(int, int, double)) &#123; int forward, backward; if(func(p, q, 20) &gt; 0)&#123; forward = 20; backward = -20; &#125; else&#123; forward = -20; backward = 20; &#125; int x = 0; while(fabs(f(p, q, x)) &gt; EPSILON)&#123; if(f(p, q, x) &gt; 0)&#123; forward = x; &#125; else backward = x; x = (backward + forward)/2; &#125; &#125;double f(int p, int q, double x) &#123; return p * x + q;&#125; 质数筛法与之前的对每一个数依次判断是否为质数的方式不同，筛法的思想是“标注出所有非质数，输出所有没被标记的数字”，声明了一个mark数组，用于标记所有质数。 123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;math.h&gt;int main() &#123; printf(&quot;2\\n&quot;); int digit; int divisor; for (digit = 3; digit &lt;= 15; digit += 2) &#123; for (divisor = 3; divisor &lt; sqrt(digit); divisor += 2) &#123; if (digit % divisor == 0)&#123; break; &#125; &#125; if (divisor == digit)&#123; printf(&quot;%d\\n&quot;, digit); &#125; &#125; return 0;&#125; 质数筛法的逻辑：对于n以内的筛选来说，如果n为合数，c为n的最小因数，1&lt; C*C &lt; n；故只要找到了c就可以确定n是合数，并将n进行标记，通过这样的一个个筛选，将容易得到的合数均筛选出去。 12345678910111213141516171819202122232425262728#include &lt;stdio.h&gt;int main() &#123; int n = 15; int mark[16] = &#123; 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 &#125;; int c; int j; for (c = 2; c * c &lt;= n; c++) &#123; if(mark[c] != 1)&#123; for(j = 2; j &lt;= n / c;j++)&#123; mark[c * j] = 1; &#125; &#125; &#125; for (c = 2; c &lt;= n; c++)&#123; if(mark[c] != 1)&#123; printf(&quot;%d\\n&quot;, c); &#125; &#125; return 0;&#125; 折半查找12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;stdio.h&gt;int BinSearch(int arr[], int len, int key)&#123; int low = 0; int high = len - 1; int mid; while(low &lt;= high)&#123; mid = (low + high) / 2; if(key == arr[mid]) return mid + 1; else if(key &gt; arr[mid]) low = mid + 1; else high = mid - 1; &#125; return 0;&#125;int main() &#123; int n; int k; int numbers[1000001]; int m; int i; int j; // 反复读入数字和查找数字的数量 while (scanf(&quot;%d%d&quot;, &amp;n, &amp;k) != EOF) &#123; // 读入给定的数字 for (i = 0; i &lt; n; i++) &#123; scanf(&quot;%d&quot;, &amp;numbers[i]); &#125; for (j = 0; j &lt; k; j++) &#123; // 读入待查找的数字， scanf(&quot;%d&quot;, &amp;m); printf(&quot;%d&quot;,BinSearch(numbers, n, m)); // 请在下面完成查找读入数字的功能 if(j &lt; k-1) printf(&quot; &quot;); &#125; &#125; return 0;&#125; 递归问题有时候递归会使时间复杂度过高，是因为使用了多次的重复计算，可以用数组来存储之前计算的值，从而避免简单的运算重复进行。经典的爬楼梯问题代码如下： 1234567891011121314#include &lt;stdio.h&gt;int main() &#123; int n, a, i; scanf(&quot;%d&quot;, &amp;n); int arr[n]; arr[0] = 0;arr[1] = 0;arr[2] = 1;arr[3] = 1; for(i = 4; i &lt;= n; i++)&#123; arr[i] = arr[i-2] + arr[i-3]; &#125; printf(&quot;%d&quot;, arr[n]); return 0;&#125; 冒泡排序基本思想：将数组中每个相邻元素进行两两比较，按照较小元素在前的原则决定是否进行交换，这样每一轮执行之后，最小元素就被换至了最后一位。完成第一轮后，我们从头进行第二轮的比较，直至倒数第二位（因为最后一位是已经被排序好的），依次进行直至所有元素被排列成预期的顺序为止。 123456//当有5个数待排序时，可写出如下的程序。for (j = 0; j &lt; 5; j++)&#123; for (i = 0; i &lt; 4 - j; i++)&#123; swap(a[i], a[i+1]); &#125; &#125; 1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;int main() &#123; int n = 10; int m; int numbers[10]; int i, j; // 读入给定的数字 for (i = 0; i &lt; n; i++) &#123; scanf(&quot;%d&quot;, &amp;numbers[i]); &#125; for (i = 0; i &lt; n; i++) for (j = 0; j &lt; n - 1 - i; j++)&#123; if(numbers[j] &lt; numbers[j+1])&#123; m = numbers[j]; numbers[j] = numbers[j+1]; numbers[j+1] = m; &#125; &#125; for(i = 0; i &lt; n; i++)&#123; printf(&quot;%d&quot;, numbers[i]); if(i != n-1)&#123; printf(&quot; &quot;); &#125; &#125; return 0;&#125; 选择排序核心思想：根据从小到大的排序需求，它每一次从到排序的数据元素中选择出最小的元素，移动至序列的起始位置，然后在剩余的待排序元素中进行排序。 用两层循环来实现：1、寻找最小的元素需要一层循环；2、逐个被选出也需要一层循环。 螺旋输出矩阵对任意的给定m行、n列的矩阵，按顺时针螺旋的顺序输出矩阵中所有的元素. 找规律，由外向内一层层进行for循环打印，最外层循环控制有多少层，每层分为（上方、右侧、下方、左侧）四个递增的循环，直至最后一层打印完；如果输出N为奇数，将会有N/2 + 1层 优化后的螺旋矩阵当然它的规律很简单，直接的方法就是先申请一个矩阵，然后按螺旋方向填入相应的元素，填充完毕后再打印出来。它的时间按复杂为O(n2)，已经是最优的（为什么？）。空间复杂度也为O(n2）。似乎已经很好了。 但是还不够好。 按照矩阵规律填充元素时，我们是随机访问矩阵元素的（如果可以按顺序访问，根本不用先存起来再打印）。随机访问内存，效率当然不高。所以即使时间复杂度已为最优，但那只是理论上的最优，在实践中表现并不一定就好。 假如能根据行列号直接计算出对应的矩阵元素就好了。当n给定后，这个矩阵就已经唯一确定了，那么每一个元素也是确定的。也就是说，每一个位置放什么元素仅仅取决于n。因此我们可以找到一个函数element(i, j)，将行号i和列号j映射成对应这个行列号的元素。当然这个函数肯定不是一个简单的函数，不是一眼就可以看出来的，但也并不是不可能。 现在我们就来考查一下这个矩阵有什么特点。注意观察一下螺旋矩阵的最外层，它的左上角的元素是最小的，然后沿顺时针方向递增，就如同一个环一样（比如n为4时，1, 2, …, 12就是最外面一层环）。再注意一下里面一层，也是一样，顺时针方向递增的一个环（比如n为4时，13, 14, 15, 16就是里面一层环）。以此类推，环里面还有一层环（n为4时有2层环，n为5时有3层环，最里面一层只有一个元素25），实际上是一个圆环套圆环结构。每一圆环最关键的元素就是左上角的那一个元素。只要知道了这个元素，再加上这个正方形环的边长就可以计算出剩下的元素。设左上角元素为a，边长为l（ell），也就是边上有几个元素，并假设左上角的行号和列号均为0，其它元素的行号和列号都以它作参考，计算方法如下所示： 1、若i == 0，element(i, j) = a + j; 2、否则若j == 0，element(i, j) = a + 4(l-4) - (i-1) - 1; 3、否则若i == l-1，element(i, j) = a + 4(l-4) - (l-2) - 1 - j; 4、否则element(i, j) = a + l - 1 + i; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&lt;iostream&gt;using namespace std; int a[10][10]; void Fun(int n)&#123; int m=1; int i,j; for(i =0;i&lt;n/2;i++)&#123; for(j=0;j&lt;n-i;j++)&#123; if(a[i][j] ==0) a[i][j] = m++; &#125; for(j=i+1;j&lt;n-i;j++)&#123; if(a[j][n-1-i] ==0) a[j][n-1-i] = m++; &#125; for(j=n-i-1;j&gt;i;j--)&#123; if(a[n-i-1][j] ==0) a[n-i-1][j] = m++; &#125; for(j=n-i-1;j&gt;i;j--)&#123; if(a[j][i] ==0) a[j][i] = m++; &#125; &#125; if(n%2==1) a[n/2][n/2]=m;&#125; int main(void)&#123; int n,i; cout&lt;&lt;&quot;请输入螺旋矩阵维数： &quot;&lt;&lt; endl; cin&gt;&gt;n; cout&lt;&lt;&quot;显示螺旋矩阵数值： &quot;&lt;&lt; endl; for(int i=0;i&lt;n;i++)&#123; for(int j=0;j&lt;n;j++)&#123; a[i][j]=0; &#125; &#125; Fun(n); for(i=0;i&lt;n;i++)&#123; for( int j=0;j&lt;n;j++)&#123; cout&lt;&lt;a[i][j]&lt;&lt; &quot;\\t&quot;; &#125; cout&lt;&lt;endl; &#125;&#125; 螺旋队列问题描述： 设1的坐标是（0，0），x方向向右为正，y方向向下为正，例如，7的坐标为（-1，-1），2的坐标为（1，0）。编程实现输入任意一点坐标（x，y），输出所对应的数字. 123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &quot;stdafx.h&quot;#include &lt;iostream&gt;#define max(a,b) ((a)&lt;(b)?(b):(a))#define abs(a) ((a)&gt;0?(a):-(a)) using namespace std; int foo(int x,int y)&#123; int t = max(abs(x),abs(y)); int u = t+t; int v = u-1; v= v*v+u; if(x == -t) v+=u+t-y; else if(y==-t) v+=3*u+x-t; else if(y ==t) v+= t-x; else v+=y-t; return v;&#125; int _tmain(int argc, _TCHAR* argv[])&#123; int x ,y; int N; cout&lt;&lt;&quot;请输入螺旋队列数字： &quot;&lt;&lt;endl; cin&gt;&gt;N; cout&lt;&lt;&quot;显示螺旋队列数值： &quot;&lt;&lt;endl; for(y=-N;y&lt;=N;y++) &#123; for(x=-N;x&lt;=N;x++) cout&lt;&lt;&quot;\\t&quot;&lt;&lt;foo(x,y); cout&lt;&lt;endl; &#125; while(scanf(&quot;%d%d&quot;,&amp;x,&amp;y)==2) //printf(&quot;%d\\n&quot;,foo(x,y)); cout&lt;&lt;&quot;\\t&quot;&lt;&lt;foo(x,y); return 0;&#125; 双螺旋矩阵","categories":[{"name":"C语言入门基础","slug":"C语言入门基础","permalink":"http://yoursite.com/categories/C%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"循环神经网络","slug":"循环神经网络","date":"2020-08-21T14:33:17.000Z","updated":"2020-08-22T08:54:54.213Z","comments":true,"path":"2020/08/21/循环神经网络/","link":"","permalink":"http://yoursite.com/2020/08/21/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"1、语言模型与多层感知机和能有效处理空间信息的卷积神经网络不同，循环神经网络是为了更好地处理时序信息而设计的。引入状态变量来存储过去的信息，并用其与当前的输入共同决定当前的输出。 语言模型：可将自然语言文本看作一段离散的时间序列，假设一段长度为T的文本中的词依次为W1、W2….Wt，那么在离散的时间序列中，Wt可看作在时间步t的输出或标签。给定一个长度为T的词的序列，语言模型将计算该序列概率：P(W1,W2,….,Wt)。为计算该语言模型，需要先计算词的概率，以及一个词在给定前几个词的情况下的条件概率。 N元语法：计算和存储多个词的概率复杂度会呈指数级增加，故通过马尔可夫假设简化语言模型：一个词的出现只与前面N个词相关，即N阶马尔可夫链。 故基于n-1阶马尔可夫链，可将语言模型改写为：$$P(W1,W2,…,Wt) = ∏P（Wt|Wt-(n-1),…,Wt-1）$$以上也叫做n元语法，当n较小时，n元语法往往不准确，当n较大时，n元语法需要计算并存储大量的词频和多词相邻频率。 2、循环神经网络循环神经网络：并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。利用之前的多层感知机，通过添加隐藏状态将其变成循环神经网络。 不含隐藏状态的多层感知机：样本数为n、输出个数为d的小批量数据样本X。设隐藏层的激活函数为f，则其输出H为：$$H = f（X Wxh + bh）$$其中隐藏层权重参数W为Rn*d，隐藏层偏差参数b为R1xh，h为隐藏单元个数。上式两者根据广播机制来相加，设输出层的输出个数为q，则输出层的输出为：$$O = HWhq +bq$$若为分类问题，可用softmax(O)来计算输出类别的概率分布。 含隐藏状态的多层感知机与上一个相区别的是，保存上一时间步的隐藏变量Ht-1，并引入一个新的权重参数Whh为Rhxh，该参数用于描述在当前时间步如何使用上一时间步的隐藏变量。$$Ht = f(Xt Wxh + Ht-1Whh +bh)$$与多层感知机相比，添加了隐藏变量来捕捉截至当前时间步的序列的历史信息，就像神经网络当前时间步的状态或记忆一样，因此也称为隐藏状态，由于在当前时间步使用了上一时间步的隐藏状态，因此计算是循环的。通过对上一次时间步的利用，其模型参数的数量不随时间步的增加而增长。将输入与前一时间步隐藏状态连结后，输入至一个激活函数为f的全连接层，该全连接层的输出就是当前时间步的隐藏状态，且模型参数为Wxh与Whh的连结，偏差为bh。 ） 3、基于字符级神经网络的语言模型 演示如何1基于当前与过去字符来预测下一个字符，训练时对每个时间步的输出层输出使用softmax运算，然后使用交叉熵损失函数来计算其与标签的误差。 处理数据集建立字符索引：将每个字符映射成一个从0开始的连续整数，又称索引以方便后续处理。为得索引，我们将数据集中所有不同字符取出来，并逐一映射到索引来构造字典。 时序数据采样：每次随机读取小批量样本和标签，样本包含连续的字符。例：时间步数为5，样本序列为5个字符：‘’想，要，有，直，升”，则其标签序列为这些字符在训练集中的下一个字符：“要，有，直，升，机”。 随机采样：每个样本为原始序列上任意截取的一段序列，相邻的两个随机小批量不一定相邻，故每次随机采样前都需要重新初始化隐藏状态。 相邻采样：令相邻的两个随机小批量在原始序列上的位置也相毗邻，此时可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态：如此循环造成的影响：1、训练模型时，只需在每个迭代周期开始时初始化隐藏状态；2、当多个小批量通过传递隐藏状态串联起来时，梯度计算将依赖串联起的序列，迭代次数增加，梯度开销会越来越大。 one-hot向量使用one-hot向量将词表示成向量输入到神经网络：每个字符已经同一个从0到N-1的连续整数值索引一一对应。如果一个字符的索引是i，则其向量为全0的长为N的向量，仅将位置为i的元素设为1。 12def to_one_hot(X,size): return [nd.one_hot(x,size) for x in X.T] 构建模型先初始化模型参数，将隐藏单元个数num_hiddens作为超参数， 1234567891011121314151617num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_sizedef get_params(): def _one(shape): return nd.random.normal(scale=o.o1, shape=shape, ctx=ctx) #先定义隐藏层参数 W_xh = _one((num_inputs, num_hiddens)) W_hh = _one((num_hiddens, num_hiddens)) b_h = nd.zeros(num_hiddens, ctx=ctx) #输出层参数 W_hq = _one((num_hiddens, num_outputs)) b_q = nd.zeros(num_outputs, ctx=ctx) #附上梯度 params = [W_xh, W_hh, b_h, W_hq, b_q] for param in params: param.attach_grad() return params 定义init_rnn_state函数来返回初始化的隐藏状态，返回由一个形状为（批量大小，隐藏单元个数）的值为0的NDArray组成的元组。 12def init_rnn_state(batch_size, num_hiddens, ctx): return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx)) 定义rnn函数来在一个时间步中计算隐藏状态与输出，激活函数使用tanh 12345678910def rnn(inputs, state, params): #inputs和outputs皆为num_steps个形状为（batch_size, vocab_size)的矩阵 W_xh, W_hh, b_h, W_hq, b_q = params H, = state outputs = [] for X in inputs: H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h) Y = nd.dot(H, W_hq) + b_q outputs.append(Y) return outputs, (H,) 定义预测函数基于前缀prefix（含有数个字符的字符串）来预测接下来的num_chars个字符: 123456789101112131415def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state, num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx): state = init_rnn_state(1, num_hiddens, ctx) output = [char_to_idx[prefix[0]]] for t in range(num_chars + len(prefix) - 1): #将上一时间步的输出作为当前时间步的输入 X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size) #计算输出和更新隐藏状态 (Y, state) = rnn(X, state, params) #下一个时间步的输入是prefix里的字符或者当前的最佳预测字符 if t &lt; len(prefix) - 1: output.append(char_to_idx[prefix[t + 1]]) else: output.append(int(Y[0].argmax(axis=1).asscalar())) return &#x27;&#x27;.join([idx_to_char[i] for i in output]) 裁剪梯度利用裁剪梯度以应对梯度爆炸，假设将所有模型参数梯度的元素拼接成一个向量g，并设裁剪的阈值为s;裁剪后的梯度的L2范数不超过s$$裁剪后的梯度为 min(s/||g|| , 1)g$$ 12345678def grad_clipping(params, theta, ctx): norm = nd.array([0], ctx) for param in params: norm += (param.grad **2).sum() norm = norm.sqrt().asscalar() if norm &gt; theta: for param in params: param.grad[:] *= theta / norm 模型训练函数困惑度用于评价语言模型的好坏，是对交叉熵损失函数做指数运算后得到的值。 与之前章节的训练函数相比，该模型训练有几点不同：1、用困惑度评价模型；2、在迭代模型参数前裁剪梯度；3、对时序数据采用不同的采样方法将导致隐藏状态初始化的不同。 使用Gluon进行模型的简洁实现Gluon的rnn模块提供了循环神经网络的实现， 时间反向传播不裁剪梯度时，模型将无法正常训练。我们将循环神经网络按时间步展开，从而得到模型变量与参数之间的依赖关系，并根据链式法则应用反向传播计算并存储梯度。 每次迭代中，我们在依次计算完以上各个梯度后，会将它们存储起来，从而避免重复计算。同时，反向传播中的梯度计算可能会依赖变量的当前值，他们正是通过正向传播计算出来的。 4、门控循环单元裁剪梯度可以应对梯度爆炸，但是没办法解决梯度衰减的问题。因此，循环神经网络在实际中很难捕捉时间序列中时间步距离较大的依赖关系。GRU门控循环神经网络通过可以学习的门来控制信息的流动。 重置门和更新门两者输入均为当前时间步输入Xt与上一时间步隐藏状态Ht-1，输出由激活函数为sigmoid函数的全连接层计算得到 候选隐藏状态GRU将计算候选隐藏状态来辅助稍后的隐藏状态计算，先将当前时间步重置门的输出与上一时间步重置门隐藏状态做按元素乘法。重置门中元素值接近0，则意味重置对应隐藏状态元素为0，则丢弃上一时间步的隐藏状态；若近似1，则保留上一时间步隐藏状态。 之后与当前时间步的输出连结，再通过含t激活函数tanh的全连接层计算候选隐藏状态，其所有元素值域为[-1,1] 隐藏状态最后，时间步的隐藏状态Ht的计算使用了当前时间步的更新门Zt来对上一时间步的隐藏状态Ht-1和当前时间步的候选隐藏状态Ht*来做组合： 在Gluon中可直接调用rnn模块中的GRU类来实现GRU门控循环。 LSTM长短期记忆LSTM引入了三个门：输入门、遗忘门、输出门 在Gluon中可调用rnn模块中的LSTM类来实现长短期记忆。 5、深度循环神经网络深度循环神经网络：目前为止介绍的循环神经网络只有一个单项的隐藏层，深度学习中通常会用到含多个隐藏层的循环神经网络，在该网络中，隐藏状态的信息不断传递至当前层的下一时间步和当前时间步的下一层。 双向循环神经网络：之前的介绍的神经网络模型都是假设当前时间步由前面的较早时间步的序列来决定，因此信息均通过隐藏状态向后传递。故双向神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"卷积神经网络","slug":"卷积神经网络","date":"2020-08-20T13:42:51.000Z","updated":"2020-08-21T14:22:34.739Z","comments":true,"path":"2020/08/20/卷积神经网络/","link":"","permalink":"http://yoursite.com/2020/08/20/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","excerpt":"","text":"1、二维卷积层二维卷积层：有高和宽两个空间维度用于处理图像数据，通常使用更直观的互相关运算来代替卷积运算。将输出与卷积核做互相关，并加上一个标量偏差来得到输出。 在训练模型时，通常先对卷积核初始化，然后不断迭代卷积核与偏差。使用corr2d函数来实现： 1234567def corr2d(X,K): h,w = K.shape Y = nd.zeros((X.shape[0] - h + 1,X.shape[1] - w + 1)) for i in range(Y.shape[0]): for j in range(Y.shape[1]): Y[i,j] = (X[ i:i+h , j: j+w]*K).sum() return Y 在构造函数中声明了weight和bias，并在forward中利用corr2d函数来实现卷积核。 1234567def __init__(self,kernel_size,**kwargs): super(Conv2D,self).__init__(**kwargs) self.weight = self.params.get(&#x27;weight&#x27;,shape=kernel_size) self.bias = self.params.get(&#x27;bias&#x27;,shape=(1,)) def forward(self,x): return corr2d(x, self.weight.data()) + self.bias.data() 一般通过数据学习得到核数组：首先构造一个卷积层，将其卷积核初始化为随机数组，并在每一次迭代中，使用平方误差来比较输出Y和卷积层的输出，并计算梯度来更新权重。 由于核数组都是学习出来的，所以卷积层使用互相关运算还是卷积运算都不影响模型预测时的输出。 特征图：输出可看作输入在空间维度上的表征。 影响X的前向计算的所有可能输入区域，叫做X的感受野。 2、填充与步幅填充：在输入高、宽的两侧填充元素；用于增加输出的高、宽。 步幅：卷积窗口从输入窗口左上方开始，依次滑动，每次滑行的行数和列数称为步幅，用于减少输出的高、宽。 3、多输入、多输出通道1、多输入时，构造输入通道数与输入数据的通道数相同的卷积核，从而能够互相关运算，对每个通道做互相关，然后通过add_n函数来进行累加 123def corr2d_multi_in(X,K): #首先沿X和K的第0维（通道维）遍历，然后用*将结果列表变为add_n函数的位置参数（positional argument）来相加 return nd.add_n(*[d2l.corr2d(x,k) for x , k in zip(X,K)]13 2、卷积核输入、输出通道数为Ci、Co，高和宽分别为Kh和Kw。若希望有多输出时，为每个通道分别创建核数组Ci x Kh x Kw，并将其在输出通道维上连结，卷积核形状为C0 X Ci x Kh x Kw，互相关时每个输出通道结果由卷积核在该通道核数组与整个输入数组计算得来。 123def corr2d_multi_in_out(X,K): #对K的第0维遍历，每次同输入X做互相关运算，所有结果用stack函数合并在一起 return nd.stack(*[corr2d_multi_in(X,K) for k in k]) 4、1 x 1 卷积层1 X 1 卷积主要发生在通道维上,输入与输出具有相同的高宽，输出中每个元素来自输入中在高、宽上相同位置的元素在不同通道之间的按权重累加。将通道维作为特征维，将高、宽上的元素当成数据样本，则其作用与全连接层等价。 1 X 1卷积层通常用来调整网络层之间的通道数，并控制模型复杂度。 5、池化层目的：为了缓解卷积层对位置的过度敏感性。 池化层：每次对输入数据的一个固定形状窗口中的元素计算输出，直接计算池化窗口内的元素最大值或平均值，也叫最大池化或者平均池化。p X q 池化层 同样池化层也能设置填充和步幅。 12pool2d = nn.MxaPool2D(1, kernel_size=(5,3),padding=(2,1),strides=(3,4))#使用高、宽分别为5，3的卷积核；在高宽上的填充数分别为2，1；高宽上步幅分别为3，4 在处理多通道输入数据时，池化层会对每个输入通道分别池化，而不是像卷积层那样将各通道输入按通道相加。因此，池化层的输出通道数与输入通道数相等。 6、卷积神经网络单隐藏层的多层感知机分类图像：将图像像素逐行展开，得到长为28*28=784的向量，并输入进全连接层中。局限性:1、同一列像素相隔远；2、对于大尺寸图像，全连接层可能导致模型过大。 卷积层如何解决：1、保留输入形状，使像素在高、宽两个方向的相关性均能被有效识别；2、通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。 LeNet模型分为卷积层块与全连接层块。 卷积层基本单位：卷积层+最大池化层。卷积层用于识别图像中空间模式，最大池化层用于降低卷积层对位置的敏感性。 卷积层输出形状为（批量大小，通道，高，宽）；当其输出传入全连接层块时，全连接层会将小批量中的每个样本变平。形状将改变为二维，第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道*高 *宽。 AlexNet模型1、包含8层变换，其中有5层卷积和2层全连接隐藏层，全连接输出个数为4096； 2、将激活函数由sigmoid改成了更简单的Relu，在不同参数初始化方法下使模型更容易训练，且在正区间的梯度恒为1; 3、利用丢弃法来控制全连接层的模型复杂度； 4、引入大量的图像增广，从而进一步扩充数据集来缓解过拟合； 5、利用了更多的卷积层和更大的参数空间来拟合大规模数据集ImageNet，是浅层网络与深度神经网络的分界线。 VGG模型VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路；其组成规律是：连续使用数个相同的填充为1、窗口形状为3X3的卷积层后接上一个步幅为2、窗口形状为2X2的最大池化层。卷积层保持输入的高、宽不变，而池化层则对其减半。使用vgg_block函数来实现基础的VGG卷积层模块，可指定卷积层数量num_convs和输出通道数num_channels。 123456def vgg_block(num_convs, num_channels): blk = nn.Sequential() for _ in range(num_convs): blk.add(nn.Conv2D(num_channels, kernel_size=3,padding=1,activation=&#x27;relu&#x27;)) blk.add(nn.MaxPool2D(pool_size=2, strides=2)) return blk VGG由卷积模块后接全连接层模块构成，卷积层串联数个vgg_block，其超参数由变量conv_arch定义，指定了VGG块中卷积层个数与输出通道数。下面构造VGG，5个卷积块，前2块用单卷积层，后3块用双卷积层。第一块输出通道为64，之后每次输出通道数翻倍，直至512。总共8个卷积，3个全连接，因此称为VGG-11。 123456789101112conv_arch = ((1,64),(1,128),(2,256),(2,512),(2,512))def vgg(conv_arch): net = nn.Sequential() #卷积层部分 for (num_convs, num_channels) in conv_arch: net.add(vgg_block(num_convs, num_channels)) #全连接层部分 net.add(nn.Dense(4096, activation=&#x27;relu&#x27;), nn.Dropout(0.5), nn.Dense(4096, activation=&#x27;relu&#x27;), nn.Dropout(0.5), nn.Dense(10)) return net NIN模型上述模型共通处均是：先以卷积层构成的模块充分抽取空间特征，再以全连接层构成的模块来输出分类结果。 NIN则提出了另外一个思路：串联多个由卷积层和全连接层构成的小网络来构建深层网络。 卷积层通常输入、输出：四维数组（样本、通道、高、宽）；全连接层输入输出：二维数组（样本、特征）。故利用1 X 1卷积层来代替全连接层，从而使空间信息自然传递至后面层。 NIN块：由一个卷积层和两个充当全连接层的1 X 1卷积层串联而来，可自由设置第一个卷积层超参数。 NIN设计:除了使用NIN块以外，NIN去掉了AlexNet最后的3个全连接层，并使用输出通道数等于标签类别数的NIN块，然后使用全局平均池化层对每个通道中所有元素求平均，并直接进行分类。NIN这个设计的好处：显著减少模型参数尺寸，从而缓解过拟合。 GoogLeNet模型GoolLeNet中的基础卷积块为Inception块，有4条并行的线路，前3条线路使用窗口分别为1X1,3X3,5X5的卷积层来抽取不同的空间尺寸下的信息，且中间2条线路会对输入先做1X1卷积来减少输入通道数，以降低模型复杂度；第四条线路则用3X3池化层后接1X1卷积层来改变通道数；且均使用了合适的填充来使输入、输出高宽一致。 GooLeNet跟VGG一样，在主体卷积部分使用5个模块，每个模块之前使用步幅为2的3 X 3最大池化层来减小输出高宽。 模块一：使用一个64通道的7X7卷积层； 模块二：使用2个卷积层，64通道的1X1卷积层，然后是将通道增大3倍的3X3卷积层，对应Inception的线路二； 模块三：串联2个完整的Inception块； 模块四：串联5个Inception块； 模块五：2个Inception块，其后紧跟输出层，故同NIN一样使用全局平均池化层来将每个通道的高、宽变成1 7、批量归一化数据标准化处理：任一特征在数据集所有样本上的均值为0、标准差为1，可以使各个特征的分布相近。对于浅层模型，数据标准化预处理已经足够了，但对于深层网络，模型参数更新仍容易造成剧烈变化。 批量归一化层（batch normalization）：为应对深层模型的挑战，在训练时，利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出数值更稳定。 全连接层的批量归一化对于全连接层：将批量归一化层放在全连接层的仿射交换与激活函数之间。 对卷积层做批量归一化对于卷积层：批量归一化发生在卷积计算之后、应用激活函数之前。 8、残差网络ResNet实践中，添加过多层后训练误差往往不降反升，即使利用批量归一化使训练深层模型更加容易，该问题仍存在。 残差块：当理想映射f(x)极接近恒等映射时，残差映射也易于捕捉恒等映射的细微波动。 ResNet沿用了VGG全3X3卷积层的设计，残差块中首先由2个相同输出通道数的3X3卷积层，每个卷积层之后接一个批量归一化层和RELU激活函数，然后将输入跳过这2个卷积运算后再加在最后的RELU激活函数前。这样设计要求2个卷积层的输入、输出形状一样，从而可以相加。 ResNet的前两层跟GoogLeNet一样，在输出通道为64、步幅为2的7X7卷积层后接步幅为2的3X3最大池化层。 1234net = nn.Sequential()net.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=3), nn.BatchNoem(),nn.Activation(&#x27;relu&#x27;), nn.MaxPool2D(pool_size=3,strides=2,padding=1)) 不同在于其每个卷积层后增加的批量归一层,GoogLeNet在后面会接4个Inception组成的模块，而ResNet则使用4个由残差块组成的模块，每个模块使用若干个同样输出通道数的残差块。我们用resnet_block函数来实现： 12345678def resnet_block(num_channels, num_residuals, first_block=False): blk = nn.Sequential() for i in range(num_residuals): if i == 0 and not first_block: blk.add(Residual(num_channels, use_1x1conv=True, strides=2)) else: blk.add(Residual(num_channels)) return blk 接着使用ResNet加入所有残差块，这里每个模块使用2个残差块 1234567net.add(resnet_block(64, 2, first_block=True), resnet_block(128,2), resnet_block(256,2), resnet_block(512,2))net.add(nn.GlobalAvgPool2D(), nn.Dense(10))#加入全局平均池化层后接上全连接层输出 9、稠密连接网络DenseNet DenseNet主要构建模块时稠密块和过渡层，前者定义了输入、输出是如何连结的，后者则用来控制通道数，使之不过大。 由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型，过度层用来控制模型复杂度。它通过1X1卷积层来减少通道数，并使用步幅为2的的平均池化层减半高、宽，从而进一步降低模型复杂度。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"MXNET机器学习初见","slug":"机器学习初见","date":"2020-08-19T15:28:46.000Z","updated":"2020-08-29T12:20:13.045Z","comments":true,"path":"2020/08/19/机器学习初见/","link":"","permalink":"http://yoursite.com/2020/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E8%A7%81/","excerpt":"","text":"1、基础知识NDArray、NumPy的广播机制：数组维度不同，后缘维度的轴长（从末尾算起的维度）相同；（4，3）+（3，）；（3，4，2）+（4，2） 2、数组维度相同，其中有个轴为1；（4，3）+（4，1）：在1轴上广播扩展。 NDArray,NumPy的相互变换： 123P = np.ones((2,3))D = nd.array(P)D.asnumpy() 自动求梯度（gradient）MXNET中使用autograd模块自动求梯度： 12345x = nd.arange(4).reshape((4,1))x.attach_grad() #申请内存with autograd.record(): y = 2 * nd.dot(x.T,x) #若y为标量，贼会默认对y元素求和后，求关于X的梯度y.backward() uniform:均匀分布采样；normal:正态分布采样；poisson:泊松分布采样。 2、线性回归损失函数：平方函数，平方损失；在模型训练中，希望找到一组模型参数为w1,w2,b使得训练样本平均损失最小。 解析解：误差最小化问题的解刚好可用数学公式表达出来；大多数为数值解，只能利用优化算法有限次迭代模型参数，从而尽可能降低损失函数的值。 全连接层：又名稠密层，输出层中的神经元与输入层中的各个输入完全连接； 矢量计算比标量逐个相加更加省时间，故往往利用矢量矩阵运算来实现深度学习； 优化算法：小批量随机梯度下降：批量大小batch size,学习率 lr 均为超参数，为人为设定并非模型学习出来的， 调参：通过反复试错来寻找合适的超参数， 123def sgd(params,lr,batch_size): #sgd函数实现小批量随机梯度下降算法 for param in params: param[:] = param - lr * param.grad / batch_size 在一个迭代周期epoch中，将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次。 Gluon简洁实现：1、提供data包来读取数据： 2、提供大量预定义层，nn模块：neural networks： 123from mxnet.gluon import nnnet = nn.Sequential() #Sequential实例是串联各层的容器，依次添加层，每一层一次计算并作为下一层输入net.add(nn.Dense(1)) #Dense全连接层，GLUON无须指定各层形状，模型会自动推断 3、利用init模块来实现模型参数初始化的各种方法： 12from mxnet.gluon import initnet.initialize(init.Normal(sigma=0.01)) #均值为0，标准差0.01的正态分布 4、定义损失函数：利用loss模块： 12from mxnet.gluon import loss as glossloss = gloss.L2Loss() #平方损失又是L2范数损失 5、定义优化算法：创建Trainer实例，以sgd作为优化算法，用来迭代net实例所有通过add函数嵌套的层包含的全部参数，可通过collect_params函数获取。 1trainer = gluon.Trainer(net.collect_params(),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:0.03&#125;) 6、训练模型：调用Trainer实例的step函数来迭代模型参数，按sgd的定义，在step中指明批量大小，从而对样本梯度求平均。 123456789num_epochs = 3for epoch in range(1,num_epochs + 1): for X,y in data_iter: with autograd.record(): l = loss(net(X),y) l.backward() trainer.step(batch_size) l = loss(net(features), labels) print(&#x27;epoach %d,loss: %f&#x27;%(epoch,l.mean().asnumpy())) 3、softmax回归模型输出为图像类别这种离散值时，使用softmax回归，其输出单元从一个变成了多个，且引入了softmax运算使输出更加适合离散值的预测和训练。 sofemax回归模型：将输出特征与权重做线性叠加，输出值个数等于标签里的类别数。例：有4种特征（4个像素的图片）和3种输出动物类别，则权重包含12个标量（带下标w)、偏差包含3个标量（带下标b）。每个On计算都依赖所有输入，故为全连接层。 softmax计算：直接用最高On作为预测输出，有2个问题。1、输出值范围不定，难以直观判断；2、误差难以衡量。softmax运算符可以解决，即归一化,但softmax运算不改变预测类别输出。$$y1 = exp(O1)/exp(O1)+exp(O2)+exp(O3)$$交叉熵函数：使用更适合衡量分布差异的测量函数，只关心对正确类别的预测概率，$$H(yi,yi) = -Σ(yilogyi)$$交叉熵损失函数，最小化其等价于最大化训练数据集所有标签类别的联合预测概率。$$l(o) =1/n * Σ(yilogy*i)$$图像分类数据集：Fashion-MNIST Gluon的DataLoader中可用多进程来加速数据读取；通过ToTensor实例将图像数据从unit8格式变换成32位浮点数格式，并除以255使得所有像素均在0至1之间。 Gluon简洁实现1、导入模块并获取函数 123456%matplotlib inlineimport d2zlzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnbatch_size = 256 #批量大小设置train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size) 2、定义和初始化模型 添加输出为10的全连接层，并用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。 123net = nn.Sequential()net.add(nn.Dense(10))net.initialize(init.Normal(sigma=0.01)) 3、同时定义softmax和交叉熵损失函数，使数值稳定性更好，使用Gluon提供的函数。 定义优化算法：使用学习率为0.1的小批量随机梯度下降算法。 12loss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params()，&#x27;sgd&#x27;,&#123;&#x27;learining_rate&#x27;:0.1&#125;) 4、使用上一节定义的训练函数来训练模型： 12num_epochs = 5d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer) 4、多层感知机深度学习主要关注多层模型，以多层感知机NLP（multilayer perceptron）为例。在单层网络的基础上引入了隐藏层hidden layer，但多个仿射线性变换叠加仍然是线性仿射，需引入非线性函数，该函数被称为激活函数 RELU函数：RELU(x) = max(x,0) sigmoid函数：sigmoid(x) = 1/[1+exp(-x)] sigmoid函数的导数：sigmoid(x)(1-sigmoid(x)) tanh（双曲正切）函数：[1- exp(-2x)]/[1+exp(-2x)] tanh函数的导数：1 - [tanh(x)]^2 Gluon的简洁实现1、导入包与模块，并定义模型,，多加一个全连接作为隐藏层，单元数为256，用RELU作为激活函数。 123456import d2zlzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnnet = nn.Sequential()net.add(nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dense(10))net.initialize(init.Normal(sigma = 0.01)) 2、使用与softmax回归几乎相同的步骤来读取数据并训练模型,学习率为0.5 123456batch_size = 256 #批量大小设置train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)oss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params()，&#x27;sgd&#x27;,&#123;&#x27;learining_rate&#x27;:0.5&#125;)num_epochs = 5 #迭代周期num_epochs指的是要循环学习几次d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer) 5、模型选择与拟合问题训练误差：模型在训练集上表现出来的误差； 泛化误差：任意一个测试数据样本上表现出的误差的期望； 使用验证数据集来进行模型选择：预留一部分在训练、测试数据集之外的数据来进行模型选择。K折交叉验证：将原始数据分成K个不重合的子数据集，做K次模型训练和验证，每一次用一个子数据集来验证模型，其他用于训练模型。最后对这K次结果分别求平均。 欠拟合：模型无法得到较低的训练误差。 过拟合：模型训练误差远小于其在测试集上误差；模型越复杂、训练集越小越容易过拟合。 6、权重衰减、丢弃法来处理过拟合权重衰减：等价于L2范数正则化，通过为模型损失函数添加惩罚项，使学到的模型参数值较小。 L2惩罚项指的是：模型权重参数的每一个元素的平方和与一个正的常数的乘积。 12def l2_penalty(w): return (w**2).sum() / 2 权重衰减Gluon简洁实现：构造Trainer实例时通过wd参数来指定权重衰减超参数，默认下会对权重、偏差同时衰减。 1234#对权重参数衰减，权重名称一般以weight结尾trainer_w = gluon.Trainer(net.collect_params(&#x27;.*weight&#x27;),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr,&#x27;wd&#x27;:wd&#125;)#不对偏差参数衰减，偏差名称一般以bias结尾trainer_b = gluon.Trainer(net.collect_params(&#x27;.*bias&#x27;),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;) 丢弃法：隐藏单元有一定的概率P被丢弃掉，丢弃概率是丢弃法的超参数。具体而言，随机变量$为0和1的概率分别为P和1-P。 定义dropout函数，以drop_prob的概率丢弃NDArray输入X中的元素。 丢弃法Gluon简洁实现：1234net = nn.Sequential()net.add(nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dropout(drop_prob1), #在第一个全连接层后添加丢弃层 nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dropout(drop_prob2),nn.Dense(10))net.initialize(init.Normal(sigma = 0.01)) 7、反向传播反向传播：指计算神经网络梯度的方法，依据链式法则，其梯度计算可能依据各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。 正向传播的计算可能依赖于模型参数的当前值，而参数是在反向传播的梯度计算后通过优化算法迭代的。 模型参数初始化完成后，交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。 梯度衰减、爆炸：由于层数过大时，输出呈幂次爆炸增长，故梯度爆炸或梯度消失。 8、深度学习计算原理细节1、基于Block类的模型构建Block类是nn模块里提供的一个模型构造类，继承Block类来构造多层感知机，重载init函数与forward函数，分别用于创建模型参数与定义前向计算。 123456789class MLP(nn.Block): #声明带有模型参数的层，声明2个全连接层 def __init__(self, **kwargs): #调用父类构造函数来进行必要初始化。 super(MLP,self).__init__(**kwargs) self.hidden = nn.Dense(256, activation=&#x27;relu&#x27;) #隐藏层 self.output = nn.Dense(10) #输出层 def forward(self,x): #定义模型的前向计算，即如何根据输入x计算返回所需的模型输出 return self.output(self.hidden(x)) 无需定义反向传播，系统将自动求梯度而生成反向传播所需的backward函数 实例化MLP类得到net，并传入输入数据X并做一次前向计算。 1234X = nd.random.uniform(shape=(2,20))net = MLP()net.initialize()net(X) 2、构建一个继承于Block类的继承类提供add函数来逐一添加串联的Block子类实例，而模型的前向计算就是将这些实例按顺序逐一计算。 1234567891011class MySequential(nn.Block): def__init__(self,**kwargs): super(MySequential,self).__init__(**kwargs) def add(self,block): #block为Block实例，当MySequential实例调用initialize函数时，系统会自动对其所有成员初始化 self._children[block.name] = block def forward(self,x): #OrderedDict保证按添加顺序遍历成员 for block in self._children.values(): x = block(x) return x 3、自定义初始化模型参数对于Sequential类构造的神经网络，可通过方括号[]来访问网络的任一层。同时Sequential实例中含模型参数的层，可通过Block类的params属性来访问该层包含的参数。 共享模型参数：在利用Block类中的forward函数里多次调用一个层来计算。或者，在构造层时指定特定的参数，若不同层使用同一份参数，则它们会在前向、反向时均共享相同的参数。 延后初始化：只有将形状是（，）的输入X传进网络做前向计算net(X)时，系统才能推断出该层权重参数形状为（，），此时才能真正地开始初始化参数。 避免延后初始化：1、要对已初始化的模型重新初始化时，由于参数形状不会变化，能够立即重新初始化；2、创建层的时候指定了它的输入个数，故系统不需要额外信息来推测参数形状。 4、自定义层5、读取与存储将内存中训练好的模型参数存储在硬盘上，供后续读取使用。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"C++初见","slug":"C-四天入门","date":"2020-08-19T10:14:31.000Z","updated":"2020-08-26T11:00:16.260Z","comments":true,"path":"2020/08/19/C-四天入门/","link":"","permalink":"http://yoursite.com/2020/08/19/C-%E5%9B%9B%E5%A4%A9%E5%85%A5%E9%97%A8/","excerpt":"","text":"基础架构1、数组与函数的关系数组与函数并无本质区别，均是一种映射的形式。变量函数，指针数组。 数组：展开的函数，关键在于指针变化，查询时间快。 函数：压缩的数组，关键在于值传递，用INDEX来定位，所用空间小。 算法中的时空互换逻辑，时间复杂度与空间复杂度可以一定程度互换。 2、C++的编程逻辑面向过程：自顶向下的编程，性能高，但需要处理实现性能的每一个细节，难以复用、扩展。 面向对象：抽象化事物，建立模型。 函数式编程：强调将计算过程分解成可复用的函数，MAP方法。 1auto add = [](int a,int b) -&gt; int &#123;return a+b&#125;; 泛型编程：STL，编写完全一般化并可重复使用的算法，其效率与针对某特定数据类型而设计的算法相同；泛型：在多种数据类型上皆可操作；将算法与数据结构完全分离。 123template &lt;typename T,typename U&gt;auto add(T a, U, b) -&gt; decltype(a+b)return a + b 3、程序执行的底层C源码—编译—》对象文件—链接—》可执行程序编译时：语法检查，一个源码生成一个目标文件 对象文件：存储各种各样定义 链接：需将所有对象文件定义捏合在一起 定义：函数具体的实现过程在这（有地址空间的为定义） 声明：说有这样一个东西（无地址空间为声明），作用于编译阶段用于语法检查，在调用函数时做语法检查，仅包含函数传入参数与返回值，并不关心函数内部 nm -C main.O 查看main.O该对象文件内容，main.O中printf由系统库实现，add由自定库实现 为何要分开定义与声明？ .h头文件—》放置声明 源文件—》放置定义把定义放在头文件往往会产生bug apt-get install vim.deb; ctrl +t 打开中断 凡是未定义（undefined）、冲突（duplicate:符号定义有2个）的错误—》一般是链接阶段的错误； 4、google测试框架要实现第三方模块功能的引入—》引入头文件 .h 其定义压缩在一起生成了库文件：静态链接库： .lib IDE：集成开发环境=文本(vim,gcc) + 编译(g++) + 调试(gdb,lldb) 添加谷歌测试框架使用make命令 一般未定义、重复定义的错误：一般均出现在链接阶段的错误。 而找不到头文件的错误，一般出现在预编译阶段，需要添加上头文件的编译路径。 1234567891011121314151617181920212223using namespace std;int add(int a, int b)&#123; return a + b&#125;TEST(test, add1)&#123; EXPECT_EQ(add(3,4),7); EXPECT_NQ(add(3,4),6); EXPECT_LT(add(3,4),8); EXPECT_LE(add(3,4),7); EXPECT_GT(add(3,4),6); EXPECT_GE(add(3,4),7);&#125;TEST(test, add2)&#123; EXPECT_EQ(add(3,4),7); EXPECT_NQ(add(3,4),7); EXPECT_LT(add(3,4),7); EXPECT_LE(add(3,4),7); EXPECT_GT(add(3,4),7); EXPECT_GE(add(3,4),7);&#125; 谷歌测试框架中Run_All：1、能够输出彩色字体；2、能够动态地获取知道有多少个测试用例 使用printf输出彩色信息使用printf输出信息之前，可以在printf添加配置参数，调整输出的字体颜色。以\\033[开头，以m结尾 12printf(&quot;\\033[1;33;41madd(3,4) = %d\\n&quot;,add(3,4));printf(&quot;hello world\\n&quot;); 两行的内容均会改变颜色，因为底层的信息决定了显示的颜色，而终端的程序就是**用来显示底层信息**的。我们上述的修改是修改了自己配置颜色信息的部分，因此Terminal看到了设置背景色、前景色之后时，之后打印均按照该信息进行，直到后续碰到谷歌框架配置颜色信息的代码时，才按照该框架的配置来显示颜色。 在该配置中0为重置所有属性，故可用下列代码确保仅一行输出改变颜色,\\033[0m 12printf(&quot;\\033[1;33;41madd(3,4) = %d\\n\\033[0m&quot;,add(3,4));printf(&quot;hello world\\n&quot;); 实现调试信息log日志打印功能在大型工程中同样的调试信息输出代码很常见，故难以判断调试信息在哪输出，因此需要能输出功能更丰富的输出调试信息。 预处理命令：宏定义#define :1、定义符号常量；2、定义傻瓜表达式；3、定义代码段 宏做的事情就是最基本的替换，发生在预处理阶段 *C源码–预处理–》待编译 + 代码 –编译–目标文件–链接–》可执行文件* g++ -E :单独执行预处理阶段 最终决定程序功能的并不是C源码，而是待编译源码。因此需要经过预处理阶段之后补充代码之后，才能够正确的实现功能。 #define:宏定义，只做基础的替换，而不做语法检查，且在编译器的视角中，宏定义一定要是一行代码。 123456789101112131415161718192021222324252627282930#define s(a,b) a*bs(int, p) = &amp;n;#这句代码是正确的，因为define并不检查语法，而是只做基础替换，替换后变为： int *p = &amp;n；s(3+6,4);#预处理后成为3+6*4；因此输出为27，而不是4*9=36#define P(a)&#123;\\ printf(&quot;%d\\n&quot;, d);\\&#125;#利用反斜杠让编译器认为上面的宏定义其实是一行代码#预定义的宏#_DATE_ 日期 #_TIME_ 时间 #_LINE_ 行号 #_FILE_ 文件名 #_func_ 函数名/非标准 #_FUNC_ 函数名/标准 #_PRETTY_FUNCTION_ 更详细的函数信息这些预定义的宏信息可用于检查版本，同样也可用于调试信息log日志打印 #define log(msg) &#123;\\ printf(&quot;[%s : %s : %d] %s\\n&quot;, __FILE__, __func__, __LINE__, msg) ;\\ &#125; 故通过log能够打印出调试信息从而进行分析如何根据上述地修改下，实现任意参数的宏，变参函数、变参宏#define log(frm, args...) &#123;\\ printf(&quot;[%s : %s : %d] %s\\n&quot;, __FILE__, __func__, __LINE__, msg) ;\\ printf(frm,##args);\\ printf(&quot;\\n&quot;);\\ &#125; 这样就实现了一个外在表现像是printf函数的宏，其输出信息会更多许多 #include：将后面文件内容原封不动地拷被至该位置，故待编译源码才能完整地反映功能。 #const：分配空间 预处理命令-条件编译如何确保在发布版中没有日志信息，即需要用简单的方式对日志信息创建开关，必须以#endif 结尾 1#ifdef DEBUG //是否定义了DEBUG宏 预处理的条件编译只有一个作用：做代码剪裁，使用该命令决定留下哪些代码 实现EXCEPT系列封装将谷歌测试框架的头文件换成自己的头文件，并编写自己头文件程序完成同样代码的编译； 1、实现TEST方法；2、实现未卜先知函数RUN_ALL_TEST；3、实现相等、不等、小于等判断方法，可判断不是由函数封装出来的，而是用宏进行封装； 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#ifndef _MYTEST_H#define _MYTEST_H#define EXPECT_EQ(a,b)&#123;\\ if(!((a) == (b))&#123;\\ print(&quot;error\\n&quot;);&#125;&#125;#define EXPECT_NE(a,b)&#123;\\ if(!((a) != (b))&#123;\\ print(&quot;error\\n&quot;);&#125;&#125;#define EXPECT_EQ(a,b)&#123;\\ if(!((a) &lt; (b))&#123;\\ print(&quot;error\\n&quot;);&#125;&#125;其实此处代码可以进行复用，来使代码变得简洁，用宏能做的基础替换来进行复用。#define EXPECT(a, comp, b)&#123;\\ if(!((a) comp (b))&#123;\\ print(&quot;error\\n&quot;);&#125;&#125;#define EXPECT_EQ(a, b) EXPECT(a, == ,b)#define EXPECT_NE(a, b) EXPECT(a, != ,b)#define EXPECT_LT(a, b) EXPECT(a, &lt; ,b) #define EXPECT_LE(a, b) EXPECT(a, &lt;= ,b)#define EXPECT_GT(a, b) EXPECT(a, &gt; ,b)#define EXPECT_GE(a, b) EXPECT(a, &gt;= ,b)TEST应当是宏，在预编译后应当展开为函数的头部信息#define TEST(a, b) void a##_##b()如何实现RUN_ALL_TEST未卜先知函数如何确保将测试用例函数信息写入存储区：注册函数#define TEST(a, b)\\void a##_##b();\\__attribute__((constructor))\\void reg_##a##_##b()&#123;\\ add_test_func(a##_##b, #a&#x27;.&#x27;#b);\\ return ;\\&#125; struct&#123; void(*func)(); const char *func_name;&#125; func_arr[100];//并非动态存储，而是只能存储100个，可用链表作用来动态开辟空间int_func_cont = 0;void add_test_func(void(*func)(), const *name)&#123; func_arr[func_cnt].func =func; func_Arr[func_cnt].func_name = name; func_cnt += 1; return ;&#125;int RUN_ALLO_TEST&#123;\\ //遍历每一个测试用例函数（如何遍历，指向函数的指针,有一个存储区存储所有测试函数用例的地址，使用struct结构来设计存储区 //依次每一个测试用例函数 for (int i = 0; i &lt; func_cntr; i++)&#123; print(&quot;[ RUN ]%S\\n&quot;,func_arr[i]-&gt;name); func_arr[i].func(); &#125; return 0; &#125;#endif 5、简单算法二分查找：在一个有序数组中查找一个数据是否存在；二分函数：二分查找如何处理浮点型数据，连续函数；二分答案 本质：二分查找解决的问题：求解单调函数F(x)，函数与数组关系，其实和有序数组查找值一样的方法； 应用特点：给出X很好求，但F(X)并不好求。对于F(X) = 2X,这种正反均好求时，用不着二分查找；而对于在数组中，给出数组下标容易得出值，但给出值查找下标较为困难。 简单版快速排序：核心：partation方法，分区方法， 头部指针、尾部指针：1、先尾部指针，找一个小于基准值放前面；2、再头部指针：找一个大于基准值的放后面；3、头尾指针依次进行交替，直至指针指向同一空位置； 写一个用于测试快速排序的程序：TEST.H 123456789101112131415161718192021222324252627#define TEST(func, arr, l, r)&#123; int *temp = (int*)malloc(sizeof(int) * n);//传入数组 for (int i = 0; i &lt;n; i++) temp[i] = arr[i];//将arr中函数拷贝到temp中 func(temp, 0 ,n-1); if(check(temp, 0, n-1))&#123; print(&quot;[ OK ]%s\\n&quot;, #func); &#125;else&#123; print(&quot;[ FAILED ]%s\\n&quot;, #func); &#125;&#125;int check(int *arr, int l, int r)&#123; for(int i = l + 1; i &lt;= r; i++)&#123; if(arr[i] &lt; arr[i-1]) return 0; &#125; return 1;&#125;int *getRandData(int n)&#123; int *arr = (int*)malloc(sizeof(int) * n);//分配位置 for (int i = 0; i &lt; n; i++) arr[i] = rand() % n; return arr;&#125;int main&#123; &#125; version1partation：选择待排序区间的第一个元素作为基准值，将小于基准值的元素放在前面，大于基准值的元素放在后面，前后指针重合时，再分别对前后两部分进行快速排序的操作。 1234567891011121314void quick_sort_v1(int *arr_, int l , int r)&#123; if(l &gt;= r) return; //递归过程第一步必然要进行边界判断 int x = 1; y = r; z = arr[];// Z为基准值应该在的位置 while(x &lt; y)&#123; while (x &lt; y &amp;&amp; arr[y] &gt;= z) --y; if(x &lt; y) arr[x++] = arr[y];//放至头指针指向的空位 while (x &lt; y &amp;&amp; arr[x] &lt;= z) ++x; if(x &lt; y) arr[y--] = arr[x]; &#125; arr[x] = z;//最后将基准值放回其原来应该在的位置 quick_sort_v1(arr, l, x - 1); quick_sort_va(arr, x + 1, r); return ;&#125; 算法工程师平时考虑的是时间复杂度吗？ 大环境下的共识：你和你身边的同事都是算法工程师，因此nlogn时间复杂度算法都能想到，关键在于在实现时代码实现的细节； version2：单边递归法当本层的快速排序做完partation操作时，会分别对左右两边进行递归操作，因此相当于一个二叉树的结构； 单边递归法核心思想：作为一个主管，在要被优化掉之前，做下一层的活。当前的version1函数，partation做完之后等着左右两边，可以让左半边继续递归，但右半边交给当前层程序进行执行。 12345678910111213141516void quick_sort_v2(int *arr_, int l , int r)&#123; while(l &lt; r)&#123; int x = l ; y = r; z = arr[l];// Z为基准值应该在的位置 while(x &lt; y)&#123; while (x &lt; y &amp;&amp; arr[y] &gt;= z) --y; if(x &lt; y) arr[x++] = arr[y];//放至头指针指向的空位 while (x &lt; y &amp;&amp; arr[x] &lt;= z) ++x; if(x &lt; y) arr[y--] = arr[x]; &#125; arr[x] = z;//最后将基准值放回其原来应该在的位置 quick_sort_v2(arr, l, x - 1); l = x + 1; &#125;//使用while，做完partation操作后，左半边排序通过递归来进行， //右半边的排序通过修改本层的 return ;&#125; version3：无监督优化凡是判断坐标范围超界的判断：均为监督项，无监督优化：将监督项干掉； 先以插入排序来举例子： 插入排序思想：将无序序列分成两部分，前半部分为已排序区，后半部分为未排序区，每次从未排序区的头部选择一个元素，插入至已排序区中。 12345678910void insert_sort_v1(int *arr, int l, int r)&#123; for (int i = l + 1; i &lt;= r; i++)&#123; int j = i; while(j &gt; 0 &amp;&amp; arr[j] &lt; arr[j - 1])&#123; swap(arr[j], arr[j-1]); --j; &#125; &#125; return ;&#125; 无监督思想：为何需要监督项，确保指针访问不越界，那么如何才能去掉这个监督项看上述while(j &gt; 0 &amp;&amp; arr[j] &lt; arr[j - 1])，只有当前插入的元素，是当前已排序区间的最小值，才会越界；因此，先做一个预处理，将该全局范围内的最小值先放置第一位，则不可能发生这种越界操作，则不再需要插入排序中的监督项。 1234567891011121314151617181920void insert_sort_v2(int *arr, int l, int r)&#123; int ind = l; for (int i = l + 1; i &lt;= r; i++)&#123; if(arr[ind] &gt; arr[i]) ind = i; &#125; //插入排序是一个稳定的排序，而直接交换两个位置的值会破坏其稳定性 //找到全局最小值位置后，应当将其依次一个个换至最前面 while (ind &gt; l)&#123; swap(arr[ind], arr[ind -1]); --ind; &#125; for (int i = l + 1; i &lt;= r; i++)&#123; int j = i; while(j &gt; 0 &amp;&amp; arr[j] &lt; arr[j - 1])&#123; swap(arr[j], arr[j-1]); --j; &#125; &#125; return ;&#125; 那么如何实现无监督的快速排序呢？快速排序中的partation过程，小于基准值放在前，大于的放在后；逻辑上讲，确定基准值后，其前后元素的数量就已经定了，由于基准值前后的位置数量是固定的，则有前一个值往后移、则必然后后面一个元素向前移。 故可以头尾指针同时向中间走，且两者同时进行交换，这样就能去掉快速排序过程中其所谓的监督项了。 12345678910111213141516void quick_sort_v3(int *arr_, int l , int r)&#123; while(l &lt; r)&#123; int x = 1, y = r, z = arr[l]; do&#123; while(arr[x] &lt; z) ++x; while(arr[y] &gt; z) --y; if(x &lt;= y)&#123; swap(arr[x], arr[y]); ++x, --y; &#125; &#125;while(x &lt;= y); quick_sort_v3(arr, l, y); l = x ; &#125; return ;&#125; 无监督的算法优化思维是一种非常重要的代码优化思维。 version4：基准值选择优化核心思维：快速排序时间复杂度，T(n) = n * h，其中h为递归二叉树的树高，而递归二叉树最多为n个节点，logn &lt; h &lt; n；则nlogn &lt; T(n) &lt; n**2 如何让快排的时间复杂度稳定在nlogn：控制二叉树树高，即每一次区分左右树时，尽量让左右两边平分，即基准值能平分数组。 方法一：三点取中法，在头指针l，尾指针r，中间元素指针m，三者指向的值之间，选取中位的那个数值作为基准值。 6、虚函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class A &#123; public : void say()&#123; cout &lt;&lt; &quot;this is Class A&quot; &lt;&lt; endl; &#125;&#125;;class B : public A &#123; public: void say()&#123; cout &lt;&lt; &quot;this is class B&quot; &lt;&lt; endl; &#125;&#125;;int main()&#123; B b; A &amp;a = b; A *c = &amp;b; b.say(); //会调用B类say方法 a.say();//会调用A类say方法 c-&gt;say();//会调用A类say方法 return 0;&#125;//普通的成员函数方法是跟着类走的，根据其类类别，来判断调用哪种成员方法//而虚函数的方法，其调用的方法是跟着对象走的class C &#123; public : virtual void say()&#123; //前面加virtual,使其变成虚函数 cout &lt;&lt; &quot;this is Class A&quot; &lt;&lt; endl; &#125;&#125;;class D : public C &#123; public: void say()&#123; //父类的方法定义为虚函数，则其继承的子类的对应方法也会变成虚函数 cout &lt;&lt; &quot;this is class B&quot; &lt;&lt; endl; &#125;&#125;;int main()&#123; B b; A &amp;a = b;//a虽然是A类型的引用，但它绑定的是B类型的对象 A *c = &amp;b;//间接引用C时，C虽然指针类型，但它指向的是B类的对象，因此也是调用B类方法 b.say(); //会调用B类say方法 a.say();//会调用A类say方法 c-&gt;say();//会调用A类say方法 return 0;&#125; 为什么虚函数可以跟着对象走：任何一个对象都会占据一片存储空间，当对象中存在虚函数时，其底层存储区域中的第一个位置会记录一个地址；该地址指向一张虚函数的表vtable，表中每一项都存储的是虚函数。 因此虚函数是跟着对象走的，不管是何种类型的对象，都会指向了虚函数表，从而调用了当前对象所绑定的虚函数方法。 假设虚函数类型为T类型，则虚函数表首地址为T* 类型， 则存储T*类型数据区的数据为T**类型。因此可用C语言中指针来提取虚函数。但若用此方法来提取带参数的虚函数时，可能会导致参数混乱。 this指针原因是：在成员方法中的特殊变量：this指针,在成员方法中，看着是一个参数，实则会添加一个隐藏参数：this指针。因此两个参数：一个是this指针指向的地址，一个才是真正的传递参数。 this指针其实是一个变量，是成员方法的隐藏参数。","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"基于HEXO的博客搭建","slug":"首篇博文","date":"2020-08-15T11:11:49.000Z","updated":"2020-08-19T15:26:07.429Z","comments":true,"path":"2020/08/15/首篇博文/","link":"","permalink":"http://yoursite.com/2020/08/15/%E9%A6%96%E7%AF%87%E5%8D%9A%E6%96%87/","excerpt":"","text":"环境准备1、Git下载并利用SSH密钥关联上GitHub账号 2、JS.node下载 3、npm、cnpm、hexo下载 创建博客并利用GitHub Pages上传网络端1、创建空文件夹，并使用hexo init 生成博客 2、利用GitHub生成新的仓库作为博客资源，在原blog文件夹下安装GIT，并部署至该仓库","categories":[{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-08-15T11:06:14.903Z","updated":"2020-08-15T11:06:14.904Z","comments":true,"path":"2020/08/15/hello-world/","link":"","permalink":"http://yoursite.com/2020/08/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"},{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"C语言入门基础","slug":"C语言入门基础","permalink":"http://yoursite.com/categories/C%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80/"},{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]}