{"meta":{"title":"Technical blog","subtitle":"IT小白的成长之旅","description":"请乐观，请珍惜","author":"Li Yudong","url":"http://yoursite.com","root":"/"},"pages":[{"title":"广告位招租","date":"2020-08-16T15:01:00.000Z","updated":"2020-08-16T15:02:08.971Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"分类","date":"2020-08-16T14:49:27.000Z","updated":"2020-08-16T14:52:47.789Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"archives","date":"2020-08-16T14:58:35.000Z","updated":"2020-08-16T14:58:35.251Z","comments":true,"path":"archives/index.html","permalink":"http://yoursite.com/archives/index.html","excerpt":"","text":""},{"title":"标签","date":"2020-08-16T14:45:00.000Z","updated":"2020-08-16T14:48:55.034Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MXNET机器学习初见","slug":"机器学习初见","date":"2020-08-19T15:28:46.000Z","updated":"2020-08-20T08:16:05.904Z","comments":true,"path":"2020/08/19/机器学习初见/","link":"","permalink":"http://yoursite.com/2020/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E8%A7%81/","excerpt":"","text":"1、基础知识NDArray、NumPy的广播机制：数组维度不同，后缘维度的轴长（从末尾算起的维度）相同；（4，3）+（3，）；（3，4，2）+（4，2） 2、数组维度相同，其中有个轴为1；（4，3）+（4，1）：在1轴上广播扩展。 NDArray,NumPy的相互变换： 123P = np.ones((2,3))D = nd.array(P)D.asnumpy() 自动求梯度（gradient）MXNET中使用autograd模块自动求梯度： 12345x = nd.arange(4).reshape((4,1))x.attach_grad() #申请内存with autograd.record(): y = 2 * nd.dot(x.T,x) #若y为标量，贼会默认对y元素求和后，求关于X的梯度y.backward() uniform:均匀分布采样；normal:正态分布采样；poisson:泊松分布采样。 2、线性回归损失函数：平方函数，平方损失；在模型训练中，希望找到一组模型参数为w1,w2,b使得训练样本平均损失最小。 解析解：误差最小化问题的解刚好可用数学公式表达出来；大多数为数值解，只能利用优化算法有限次迭代模型参数，从而尽可能降低损失函数的值。 全连接层：又名稠密层，输出层中的神经元与输入层中的各个输入完全连接； 矢量计算比标量逐个相加更加省时间，故往往利用矢量矩阵运算来实现深度学习； 优化算法：小批量随机梯度下降：批量大小batch size,学习率 lr 均为超参数，为人为设定并非模型学习出来的， 调参：通过反复试错来寻找合适的超参数， 123def sgd(params,lr,batch_size): #sgd函数实现小批量随机梯度下降算法 for param in params: param[:] = param - lr * param.grad / batch_size 在一个迭代周期epoch中，将完整遍历一遍data_iter函数，并对训练数据集中所有样本都使用一次。 Gluon简洁实现：1、提供data包来读取数据： 2、提供大量预定义层，nn模块：neural networks： 123from mxnet.gluon import nnnet = nn.Sequential() #Sequential实例是串联各层的容器，依次添加层，每一层一次计算并作为下一层输入net.add(nn.Dense(1)) #Dense全连接层，GLUON无须指定各层形状，模型会自动推断 3、利用init模块来实现模型参数初始化的各种方法： 12from mxnet.gluon import initnet.initialize(init.Normal(sigma=0.01)) #均值为0，标准差0.01的正态分布 4、定义损失函数：利用loss模块： 12from mxnet.gluon import loss as glossloss = gloss.L2Loss() #平方损失又是L2范数损失 5、定义优化算法：创建Trainer实例，以sgd作为优化算法，用来迭代net实例所有通过add函数嵌套的层包含的全部参数，可通过collect_params函数获取。 1trainer = gluon.Trainer(net.collect_params(),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:0.03&#125;) 6、训练模型：调用Trainer实例的step函数来迭代模型参数，按sgd的定义，在step中指明批量大小，从而对样本梯度求平均。 123456789num_epochs = 3for epoch in range(1,num_epochs + 1): for X,y in data_iter: with autograd.record(): l = loss(net(X),y) l.backward() trainer.step(batch_size) l = loss(net(features), labels) print(&#x27;epoach %d,loss: %f&#x27;%(epoch,l.mean().asnumpy())) 3、softmax回归模型输出为图像类别这种离散值时，使用softmax回归，其输出单元从一个变成了多个，且引入了softmax运算使输出更加适合离散值的预测和训练。 sofemax回归模型：将输出特征与权重做线性叠加，输出值个数等于标签里的类别数。例：有4种特征（4个像素的图片）和3种输出动物类别，则权重包含12个标量（带下标w)、偏差包含3个标量（带下标b）。每个On计算都依赖所有输入，故为全连接层。 softmax计算：直接用最高On作为预测输出，有2个问题。1、输出值范围不定，难以直观判断；2、误差难以衡量。softmax运算符可以解决，即归一化,但softmax运算不改变预测类别输出。$$y1 = exp(O1)/exp(O1)+exp(O2)+exp(O3)$$交叉熵函数：使用更适合衡量分布差异的测量函数，只关心对正确类别的预测概率，$$H(yi,yi) = -Σ(yilogyi)$$交叉熵损失函数，最小化其等价于最大化训练数据集所有标签类别的联合预测概率。$$l(o) =1/n * Σ(yilogy*i)$$图像分类数据集：Fashion-MNIST Gluon的DataLoader中可用多进程来加速数据读取；通过ToTensor实例将图像数据从unit8格式变换成32位浮点数格式，并除以255使得所有像素均在0至1之间。 Gluon简洁实现1、导入模块并获取函数 123456%matplotlib inlineimport d2zlzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnbatch_size = 256 #批量大小设置train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size) 2、定义和初始化模型 添加输出为10的全连接层，并用均值为0、标准差为0.01的正态分布随机初始化模型的权重参数。 123net = nn.Sequential()net.add(nn.Dense(10))net.initialize(init.Normal(sigma=0.01)) 3、同时定义softmax和交叉熵损失函数，使数值稳定性更好，使用Gluon提供的函数。 定义优化算法：使用学习率为0.1的小批量随机梯度下降算法。 12loss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params()，&#x27;sgd&#x27;,&#123;&#x27;learining_rate&#x27;:0.1&#125;) 4、使用上一节定义的训练函数来训练模型： 12num_epochs = 5d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer) 4、多层感知机深度学习主要关注多层模型，以多层感知机NLP（multilayer perceptron）为例。在单层网络的基础上引入了隐藏层hidden layer，但多个仿射线性变换叠加仍然是线性仿射，需引入非线性函数，该函数被称为激活函数 RELU函数：RELU(x) = max(x,0) sigmoid函数：sigmoid(x) = 1/[1+exp(-x)] sigmoid函数的导数：sigmoid(x)(1-sigmoid(x)) tanh（双曲正切）函数：[1- exp(-2x)]/[1+exp(-2x)] tanh函数的导数：1 - [tanh(x)]^2 Gluon的简洁实现1、导入包与模块，并定义模型,，多加一个全连接作为隐藏层，单元数为256，用RELU作为激活函数。 123456import d2zlzh as d2lfrom mxnet import gluon, initfrom mxnet.gluon import loss as gloss, nnnet = nn.Sequential()net.add(nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dense(10))net.initialize(init.Normal(sigma = 0.01)) 2、使用与softmax回归几乎相同的步骤来读取数据并训练模型,学习率为0.5 123456batch_size = 256 #批量大小设置train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)oss = gloss.SoftmaxCrossEntropyLoss()trainer = gluon.Trainer(net.collect_params()，&#x27;sgd&#x27;,&#123;&#x27;learining_rate&#x27;:0.5&#125;)num_epochs = 5 #迭代周期num_epochs指的是要循环学习几次d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer) 5、模型选择与拟合问题训练误差：模型在训练集上表现出来的误差； 泛化误差：任意一个测试数据样本上表现出的误差的期望； 使用验证数据集来进行模型选择：预留一部分在训练、测试数据集之外的数据来进行模型选择。K折交叉验证：将原始数据分成K个不重合的子数据集，做K次模型训练和验证，每一次用一个子数据集来验证模型，其他用于训练模型。最后对这K次结果分别求平均。 欠拟合：模型无法得到较低的训练误差。 过拟合：模型训练误差远小于其在测试集上误差；模型越复杂、训练集越小越容易过拟合。 6、权重衰减、丢弃法来处理过拟合权重衰减：等价于L2范数正则化，通过为模型损失函数添加惩罚项，使学到的模型参数值较小。 L2惩罚项指的是：模型权重参数的每一个元素的平方和与一个正的常数的乘积。 12def l2_penalty(w): return (w**2).sum() / 2 权重衰减Gluon简洁实现：构造Trainer实例时通过wd参数来指定权重衰减超参数，默认下会对权重、偏差同时衰减。 1234#对权重参数衰减，权重名称一般以weight结尾trainer_w = gluon.Trainer(net.collect_params(&#x27;.*weight&#x27;),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr,&#x27;wd&#x27;:wd&#125;)#不对偏差参数衰减，偏差名称一般以bias结尾trainer_b = gluon.Trainer(net.collect_params(&#x27;.*bias&#x27;),&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;) 丢弃法：隐藏单元有一定的概率P被丢弃掉，丢弃概率是丢弃法的超参数。具体而言，随机变量$为0和1的概率分别为P和1-P。 定义dropout函数，以drop_prob的概率丢弃NDArray输入X中的元素。 丢弃法Gluon简洁实现：1234net = nn.Sequential()net.add(nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dropout(drop_prob1), #在第一个全连接层后添加丢弃层 nn.Dense(256,activation = &#x27;relu&#x27;),nn.Dropout(drop_prob2),nn.Dense(10))net.initialize(init.Normal(sigma = 0.01)) 7、反向传播反向传播：指计算神经网络梯度的方法，依据链式法则，其梯度计算可能依据各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。 正向传播的计算可能依赖于模型参数的当前值，而参数是在反向传播的梯度计算后通过优化算法迭代的。 模型参数初始化完成后，交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。 梯度衰减、爆炸：由于层数过大时，输出呈幂次爆炸增长，故梯度爆炸或梯度消失。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"C++四天入门","slug":"C-四天入门","date":"2020-08-19T10:14:31.000Z","updated":"2020-08-19T15:22:03.380Z","comments":true,"path":"2020/08/19/C-四天入门/","link":"","permalink":"http://yoursite.com/2020/08/19/C-%E5%9B%9B%E5%A4%A9%E5%85%A5%E9%97%A8/","excerpt":"","text":"Day One 基础架构1、数组与函数的关系数组与函数并无本质区别，均是一种映射的形式。变量函数，指针数组。 数组：展开的函数，关键在于指针变化，查询时间快。 函数：压缩的数组，关键在于值传递，用INDEX来定位，所用空间小。 算法中的时空互换逻辑，时间复杂度与空间复杂度可以一定程度互换。 2、C++的编程逻辑面向过程：自顶向下的编程，性能高，但需要处理实现性能的每一个细节，难以复用、扩展。 面向对象：抽象化事物，建立模型。 函数式编程：强调将计算过程分解成可复用的函数，MAP方法。 1auto add = [](int a,int b) -&gt; int &#123;return a+b&#125;; 泛型编程：STL，编写完全一般化并可重复使用的算法，其效率与针对某特定数据类型而设计的算法相同；泛型：在多种数据类型上皆可操作；将算法与数据结构完全分离。 123template &lt;typename T,typename U&gt;auto add(T a, U, b) -&gt; decltype(a+b)return a + b 3、程序执行的底层C源码—编译—》对象文件—链接—》可执行程序 编译时：语法检查，一个源码生成一个目标文件 对象文件：存储各种各样定义 链接：需将所有对象文件定义捏合在一起 定义：函数具体的实现过程在这（有地址空间的为定义） 声明：说有这样一个东西（无地址空间为声明），作用于编译阶段用于语法检查，在调用函数时做语法检查，仅包含函数传入参数与返回值，并不关心函数内部 nm -C main.O 查看main.O该对象文件内容，main.O中printf由系统库实现，add由自定库实现 为何要分开定义与声明？ .h头文件—》放置声明 源文件—》放置定义；把定义放在头文件往往会产生bug apt-get install vim.deb; ctrl +t 打开中断 凡是未定义（undefined）、冲突（duplicate:符号定义有2个）的错误—》一般是链接阶段的错误； 4、google测试框架要实现第三方模块功能的引入—》引入头文件 .h 其定义压缩在一起生成了库文件：静态链接库： .lib IDE：集成开发环境=文本(vim,gcc) + 编译(g++) + 调试(gdb,lldb)","categories":[{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"基于HEXO的博客搭建","slug":"首篇博文","date":"2020-08-15T11:11:49.000Z","updated":"2020-08-19T15:26:07.429Z","comments":true,"path":"2020/08/15/首篇博文/","link":"","permalink":"http://yoursite.com/2020/08/15/%E9%A6%96%E7%AF%87%E5%8D%9A%E6%96%87/","excerpt":"","text":"环境准备1、Git下载并利用SSH密钥关联上GitHub账号 2、JS.node下载 3、npm、cnpm、hexo下载 创建博客并利用GitHub Pages上传网络端1、创建空文件夹，并使用hexo init 生成博客 2、利用GitHub生成新的仓库作为博客资源，在原blog文件夹下安装GIT，并部署至该仓库","categories":[{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]},{"title":"Hello World","slug":"hello-world","date":"2020-08-15T11:06:14.903Z","updated":"2020-08-15T11:06:14.904Z","comments":true,"path":"2020/08/15/hello-world/","link":"","permalink":"http://yoursite.com/2020/08/15/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"C++","slug":"C","permalink":"http://yoursite.com/categories/C/"},{"name":"前端页面","slug":"前端页面","permalink":"http://yoursite.com/categories/%E5%89%8D%E7%AB%AF%E9%A1%B5%E9%9D%A2/"}],"tags":[{"name":"初学者","slug":"初学者","permalink":"http://yoursite.com/tags/%E5%88%9D%E5%AD%A6%E8%80%85/"}]}